{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c005a7",
   "metadata": {},
   "source": [
    "# Ernesto Antonio Reyes Ramírez \n",
    "\n",
    "# Procesamiento de Lenguaje Natural \n",
    "\n",
    "# Tarea 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c3171",
   "metadata": {},
   "source": [
    "# Modelo de Lenguaje Neuronales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "21f28a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "from typing import Tuple\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "#Preprocesing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "#Scikit-learn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68db25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1111\n",
    "random.seed(seed) \n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124fd889",
   "metadata": {},
   "source": [
    "### Procesamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62848dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"mex_train.txt\", sep = \"\\r\\n\",engine = \"python\",header = None).loc[:,0].values.tolist()\n",
    "X_val = pd.read_csv(\"mex_val.txt\", sep = \"\\r\\n\",engine = \"python\",header = None).loc[:,0].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8a7e18",
   "metadata": {},
   "source": [
    "### Clases y funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e06396a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramData():\n",
    "    \n",
    "    def __init__(self,N: int,vocab_max: int = 5000, tokenizer = None, embeddings_model = None, nivel = \"palabra\"):\n",
    "        self.tokenizer = tokenizer if tokenizer else self.default_tokenizer\n",
    "        self.punct = set(['.',',',';',':','-','^','!','¡','¿','?','\"','...','<url>','*','@usuario','»','\\''])\n",
    "        self.N = N\n",
    "        self.vocab_max = vocab_max\n",
    "        self.UNK = \"<unk>\"\n",
    "        self.SOS = \"<s>\"\n",
    "        self.EOS = \"</s>\"\n",
    "        self.embeddings_model = embeddings_model\n",
    "        self.nivel = nivel\n",
    "        \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def default_tokenizer(self,doc:str) -> list:\n",
    "        return doc.split(\" \")\n",
    "    \n",
    "    def remove_word(self,word:str) ->bool:\n",
    "        word = word.lower()\n",
    "        is_punct = True if word in self.punct else False\n",
    "        is_digit = word.isnumeric()\n",
    "        return is_punct or is_digit\n",
    "    \n",
    "    def sortFreqDict(self,freq_dist) -> list:\n",
    "        freq_dict = dict(freq_dist)\n",
    "        return sorted(freq_dict,key = freq_dict.get,reverse = True)\n",
    "    \n",
    "    def get_vocab(self,corpus:list) -> set:\n",
    "        if self.nivel == \"palabra\":\n",
    "            freq_dist = FreqDist([w.lower() for sentence in corpus for w in self.tokenizer(sentence) if not self.remove_word(w)])\n",
    "            sorted_words = self.sortFreqDict(freq_dist)[:self.vocab_max-3]\n",
    "        else:\n",
    "            freq_dist = FreqDist([w.lower() for sentence in corpus for w in list(sentence) if not self.remove_word(w)])\n",
    "            sorted_words = self.sortFreqDict(freq_dist)\n",
    "        return set(sorted_words)\n",
    "    \n",
    "    def fit(self,corpus: list) -> None:\n",
    "        self.vocab = self.get_vocab(corpus)\n",
    "        self.vocab.add(self.UNK)\n",
    "        self.vocab.add(self.SOS)\n",
    "        self.vocab.add(self.EOS)\n",
    "        \n",
    "        self.w2id = {}\n",
    "        self.id2w = {}\n",
    "        \n",
    "        if self.embeddings_model is not None:\n",
    "            primer_valor = list(self.embeddings_model.values())[0]\n",
    "            longitud_primer_valor = len(primer_valor)\n",
    "            self.embedding_matrix = np.empty([len(self.vocab),longitud_primer_valor])\n",
    "            \n",
    "        id = 0 \n",
    "        for doc in corpus:\n",
    "            if self.nivel == \"palabra\":\n",
    "                new_doc = self.tokenizer(doc)\n",
    "            else:\n",
    "                new_doc = list(doc)\n",
    "            for word in new_doc:\n",
    "                word_ = word.lower()\n",
    "                if word_ in self.vocab and not word_ in self.w2id:\n",
    "                    self.w2id[word_] = id\n",
    "                    self.id2w[id] = word_\n",
    "                    \n",
    "                    if self.embeddings_model is not None:\n",
    "                        if word_ in self.embeddings_model:\n",
    "                            self.embedding_matrix[id] = self.embeddings_model[word_]\n",
    "                        else:\n",
    "                            primer_valor = list(self.embeddings_model.values())[0]\n",
    "                            longitud_primer_valor = len(primer_valor)\n",
    "                            self.embedding_matrix[id] = np.random.rand(longitud_primer_valor)\n",
    "                    \n",
    "                    id += 1\n",
    "        \n",
    "        \n",
    "        #tokens especiales\n",
    "        \n",
    "        self.w2id.update(\n",
    "            {\n",
    "                self.UNK: id,\n",
    "                self.SOS: id+1,\n",
    "                self.EOS: id+2\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.id2w.update(\n",
    "            {\n",
    "                id: self.UNK,\n",
    "                id+1: self.SOS,\n",
    "                id+2: self.EOS\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def transform(self,corpus:list) -> Tuple[np.ndarray,np.ndarray]:\n",
    "        \n",
    "        X_ngrams = []\n",
    "        y = []         \n",
    "            \n",
    "        for doc in corpus:\n",
    "            doc_ngram = self.get_ngram_doc(doc)\n",
    "            for words_window in doc_ngram:\n",
    "                words_window_ids = [self.w2id[w] for w in words_window]\n",
    "                X_ngrams.append(list(words_window_ids[:-1]))\n",
    "                y.append(words_window_ids[-1])\n",
    "                \n",
    "        \n",
    "        return np.array(X_ngrams),np.array(y)\n",
    "    \n",
    "    def get_ngram_doc(self,doc:str) -> list:\n",
    "        if self.nivel == \"palabra\":\n",
    "            doc_tokens = self.tokenizer(doc)\n",
    "        else:\n",
    "            doc_tokens = list(doc)\n",
    "        doc_tokens = self.replace_unk(doc_tokens)\n",
    "        doc_tokens = [w.lower() for w in doc_tokens]\n",
    "        doc_tokens = [self.SOS]*(self.N-1)+ doc_tokens + [self.EOS]\n",
    "        return list(ngrams(doc_tokens,self.N))\n",
    "    \n",
    "    def replace_unk(self,doc_tokens:list) -> list:\n",
    "        for i,token in enumerate(doc_tokens):\n",
    "            if token.lower() not in self.vocab:\n",
    "                doc_tokens[i] = self.UNK\n",
    "        \n",
    "        return doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eed58dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM( nn.Module ):\n",
    "    \n",
    "    def __init__(self,args, embeddings = None):\n",
    "        super(NeuralLM,self).__init__()\n",
    "        \n",
    "        self.window_size = args.N-1\n",
    "        self.embedding_dim = args.d\n",
    "        if embeddings is not None:\n",
    "            self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "            for i in range(embeddings.shape[0]):\n",
    "                for j in range(embeddings.shape[1]):\n",
    "                    self.emb.weight.data[i][j] = embeddings[i][j]\n",
    "                    \n",
    "        else :\n",
    "            self.emb   = nn.Embedding(args.vocab_size, args.d)\n",
    "        \n",
    "        self.fc1 = nn.Linear(args.d*(args.N-1),args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h,args.vocab_size,bias=False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1,self.window_size*self.embedding_dim)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42a4f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(raw_logits):\n",
    "    probs = F.softmax(raw_logits.detach(),dim=1)\n",
    "    y_pred = torch.argmax(probs,dim=1).cpu().numpy()\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2672d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(data,model,gpu=False):\n",
    "    with torch.no_grad():\n",
    "        preds,tgts = [],[]\n",
    "        for window_words, labels in data:\n",
    "            if gpu:\n",
    "                window_words = window_words.cuda()\n",
    "                \n",
    "            outputs = model(window_words)\n",
    "            \n",
    "            y_pred = get_preds(outputs)\n",
    "            tgt = labels.numpy()\n",
    "            tgts.append(tgt)\n",
    "            preds.append(y_pred)\n",
    "            \n",
    "    tgts = [e for l in tgts for e in l]\n",
    "    preds = [e for l in preds for e in l]\n",
    "    \n",
    "    return accuracy_score(tgts,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1741442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state,is_best,checkpoint_path, filename=\"checkpoint.pt\"):\n",
    "    filename = os.path.join(checkpoint_path,filename)\n",
    "    torch.save(state,filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename,os.path.join(checkpoint_path, \"model_best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "561b3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_closest_words(embeddings,ngram_data,word,n):\n",
    "    word_id = torch.LongTensor([ngram_data.w2id[word]])\n",
    "    word_embed = embeddings(word_id)\n",
    "    dists = torch.norm(embeddings.weight - word_embed,dim=1).detach()\n",
    "    lst = sorted(enumerate(dists.numpy()), key = lambda x:x[1])\n",
    "    for idx, difference in lst[1:n+1]:\n",
    "        print(ngram_data.id2w[idx],difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "937037a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text(text,tokenizer,nivel = \"palabra\"):\n",
    "    if nivel == \"palabra\":\n",
    "        all_tokens = [w.lower() if w in ngram_data.w2id else \"<unk>\" for w in tokenizer.tokenize(text)]\n",
    "        token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    else:\n",
    "        all_tokens = [w.lower() if w in ngram_data.w2id else \"<unk>\" for w in list(text)]\n",
    "        token_ids = [ngram_data.w2id[word.lower()] for word in all_tokens]\n",
    "    return all_tokens, token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "148c6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_next_word(logits, temperature = 1.0):\n",
    "    logits = np.asarray(logits).astype(\"float64\")\n",
    "    preds = logits/temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds/np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dbd2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_token(model, token_ids):\n",
    "    word_ids_tensor = torch.LongTensor(token_ids).unsqueeze(0)\n",
    "    y_raw_pred = model(word_ids_tensor).squeeze(0).detach().numpy()\n",
    "    \n",
    "    y_pred = sample_next_word(y_raw_pred,1.0)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a5afd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(model, initial_text, tokenizer,nivel = \"palabra\"):\n",
    "    all_tokens, window_word_ids = parse_text(initial_text,tokenizer,nivel)\n",
    "    \n",
    "    for i in range(300):\n",
    "        y_pred = predict_next_token(model, window_word_ids)\n",
    "        next_word = ngram_data.id2w[y_pred]\n",
    "        all_tokens.append(next_word)\n",
    "        \n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        else:\n",
    "            window_word_ids.pop(0)\n",
    "            window_word_ids.append(y_pred)\n",
    "    \n",
    "    return \" \".join(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86309431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(model, text, ngram_model):\n",
    "    \n",
    "    X,y = ngram_data.transform([text])\n",
    "    \n",
    "    X,y = X[2:],y[2:]\n",
    "    X = torch.LongTensor(X).unsqueeze(0)\n",
    "    \n",
    "    logits = model(X).detach()\n",
    "    probs = F.softmax(logits, dim=1).numpy()\n",
    "    \n",
    "    return np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "13be43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(model, text, ngram_model):\n",
    "    X, y   = ngram_data.transform(text)\n",
    "    X, y   = X[2:], y[2:]\n",
    "    X      = torch.LongTensor(X).unsqueeze(0)\n",
    "  \n",
    "    logits = model(X).detach()\n",
    "    probs  = F.softmax(logits, dim = 1).numpy()\n",
    "  \n",
    "    return - 1.0 / len(text) * np.sum([np.log(probs[i][w]) for i, w in enumerate(y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66371d00",
   "metadata": {},
   "source": [
    "# 1. Modelo basado en caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "273dd8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4a4cbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "ngram_data = NgramData(args.N, 5000, tk.tokenize, nivel = \"caracter\")\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "575a24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 345\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocab size: {ngram_data.get_vocab_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "91007326",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val     = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6dd534b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observations: X:(498949, 5), y: (498949,) \n",
      "Validation observations: X:(54110, 5), y: (54110,) \n"
     ]
    }
   ],
   "source": [
    "print(f'Training observations: X:{X_ngram_train.shape}, y: {y_ngram_train.shape} ')\n",
    "print(f'Validation observations: X:{X_ngram_val.shape}, y: {y_ngram_val.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "52680729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', '<s>', '<s>', 'l'],\n",
       " ['<s>', '<s>', '<s>', 'l', 'o'],\n",
       " ['<s>', '<s>', 'l', 'o', ' '],\n",
       " ['<s>', 'l', 'o', ' ', 'p'],\n",
       " ['l', 'o', ' ', 'p', 'e'],\n",
       " ['o', ' ', 'p', 'e', 'o'],\n",
       " [' ', 'p', 'e', 'o', 'r'],\n",
       " ['p', 'e', 'o', 'r', ' '],\n",
       " ['e', 'o', 'r', ' ', 'd'],\n",
       " ['o', 'r', ' ', 'd', 'e'],\n",
       " ['r', ' ', 'd', 'e', ' '],\n",
       " [' ', 'd', 'e', ' ', 't'],\n",
       " ['d', 'e', ' ', 't', 'o'],\n",
       " ['e', ' ', 't', 'o', 'd'],\n",
       " [' ', 't', 'o', 'd', 'o'],\n",
       " ['t', 'o', 'd', 'o', ' '],\n",
       " ['o', 'd', 'o', ' ', 'e'],\n",
       " ['d', 'o', ' ', 'e', 's'],\n",
       " ['o', ' ', 'e', 's', ' '],\n",
       " [' ', 'e', 's', ' ', 'q'],\n",
       " ['e', 's', ' ', 'q', 'u'],\n",
       " ['s', ' ', 'q', 'u', 'e'],\n",
       " [' ', 'q', 'u', 'e', ' ']]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[ngram_data.id2w[w] for w in tw] for tw in X_ngram_train[:24]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ff34873d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'p',\n",
       " 'e',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " 'd',\n",
       " 'o',\n",
       " ' ',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'q',\n",
       " 'u',\n",
       " 'e']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ngram_data.id2w[w] for w in y_ngram_train[:22]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "30f8154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size in args\n",
    "args.batch_size = 64\n",
    "\n",
    "# Num workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
    "train_loader  = DataLoader(train_dataset, \n",
    "                           batch_size = args.batch_size,\n",
    "                           num_workers = args.num_workers,\n",
    "                           shuffle     = True)\n",
    "# Val\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
    "                              torch.tensor(y_ngram_val, dtype = torch.int64))\n",
    "val_loader  = DataLoader(val_dataset, \n",
    "                           batch_size  = args.batch_size,\n",
    "                           num_workers = args.num_workers,\n",
    "                           shuffle     = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "aedfa052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : torch.Size([64, 5])\n",
      "y shape : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape : {batch[0].shape}')\n",
    "print(f'y shape : {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "7f321088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametros de la red\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.d = 100  #dimension of word embeddings\n",
    "args.d_h = 200  #dimension for hidden layer\n",
    "args.dropout = 0.1\n",
    "\n",
    "#parametros de entrenamiento\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "args.lr_patience = 10\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#saving directory\n",
    "args.savedir = \"model\"\n",
    "os.makedirs(args.savedir,exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args)\n",
    "\n",
    "#send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\"min\",\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e61a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.42481884057971014\n",
      "Epoch [1/100], Loss: 1.9675 - Val accuracy: 0.4481 - Epoch time: 25.28\n",
      "Train acc: 0.4612278761061947\n",
      "Epoch [2/100], Loss: 1.8122 - Val accuracy: 0.4652 - Epoch time: 19.39\n",
      "Train acc: 0.47452626010003846\n",
      "Epoch [3/100], Loss: 1.7636 - Val accuracy: 0.4311 - Epoch time: 18.22\n",
      "Train acc: 0.481227154674875\n",
      "Epoch [4/100], Loss: 1.7342 - Val accuracy: 0.4771 - Epoch time: 17.99\n",
      "Train acc: 0.4864807778632807\n",
      "Epoch [5/100], Loss: 1.7149 - Val accuracy: 0.4856 - Epoch time: 18.06\n",
      "Train acc: 0.48943944786456334\n",
      "Epoch [6/100], Loss: 1.6984 - Val accuracy: 0.4891 - Epoch time: 18.00\n",
      "Train acc: 0.4927291746825702\n",
      "Epoch [7/100], Loss: 1.6874 - Val accuracy: 0.4812 - Epoch time: 18.06\n",
      "Train acc: 0.4949367545209696\n",
      "Epoch [8/100], Loss: 1.6783 - Val accuracy: 0.4906 - Epoch time: 18.00\n",
      "Train acc: 0.49768220148775166\n",
      "Epoch [9/100], Loss: 1.6686 - Val accuracy: 0.4924 - Epoch time: 17.87\n",
      "Train acc: 0.49935512055918946\n",
      "Epoch [10/100], Loss: 1.6624 - Val accuracy: 0.4898 - Epoch time: 18.41\n",
      "Train acc: 0.5009647139925613\n",
      "Epoch [11/100], Loss: 1.6560 - Val accuracy: 0.4835 - Epoch time: 17.97\n",
      "Train acc: 0.5016961651917404\n",
      "Epoch [12/100], Loss: 1.6506 - Val accuracy: 0.4946 - Epoch time: 18.12\n",
      "Train acc: 0.5026304187508016\n",
      "Epoch [13/100], Loss: 1.6468 - Val accuracy: 0.4952 - Epoch time: 18.28\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.5039426221623702\n",
      "Epoch [14/100], Loss: 1.6421 - Val accuracy: 0.4967 - Epoch time: 18.23\n",
      "Train acc: 0.5139725214826215\n",
      "Epoch [15/100], Loss: 1.6059 - Val accuracy: 0.5087 - Epoch time: 18.93\n",
      "Train acc: 0.5147400442477876\n",
      "Epoch [16/100], Loss: 1.6000 - Val accuracy: 0.5091 - Epoch time: 17.99\n",
      "Train acc: 0.5151264107990253\n",
      "Epoch [17/100], Loss: 1.5985 - Val accuracy: 0.5084 - Epoch time: 18.00\n",
      "Train acc: 0.5155179876875722\n",
      "Epoch [18/100], Loss: 1.5972 - Val accuracy: 0.5095 - Epoch time: 17.96\n",
      "Train acc: 0.5161893196101065\n",
      "Epoch [19/100], Loss: 1.5954 - Val accuracy: 0.5118 - Epoch time: 18.38\n",
      "Train acc: 0.5165961267154033\n",
      "Epoch [20/100], Loss: 1.5935 - Val accuracy: 0.5091 - Epoch time: 18.07\n",
      "Train acc: 0.5175275747082211\n",
      "Epoch [21/100], Loss: 1.5927 - Val accuracy: 0.5090 - Epoch time: 18.04\n",
      "Train acc: 0.5176778728998332\n",
      "Epoch [22/100], Loss: 1.5924 - Val accuracy: 0.5091 - Epoch time: 18.14\n",
      "Train acc: 0.5174598403232012\n",
      "Epoch [23/100], Loss: 1.5911 - Val accuracy: 0.5095 - Epoch time: 17.90\n",
      "Train acc: 0.5178165480312941\n",
      "Epoch [24/100], Loss: 1.5903 - Val accuracy: 0.5108 - Epoch time: 17.93\n",
      "Epoch 00025: reducing learning rate of group 0 to 5.7500e-02.\n",
      "Train acc: 0.5177688534051558\n",
      "Epoch [25/100], Loss: 1.5898 - Val accuracy: 0.5106 - Epoch time: 17.96\n",
      "Train acc: 0.5229387104014365\n",
      "Epoch [26/100], Loss: 1.5712 - Val accuracy: 0.5161 - Epoch time: 18.03\n",
      "Train acc: 0.5237527254072079\n",
      "Epoch [27/100], Loss: 1.5684 - Val accuracy: 0.5145 - Epoch time: 18.06\n",
      "Train acc: 0.5241583301269719\n",
      "Epoch [28/100], Loss: 1.5668 - Val accuracy: 0.5167 - Epoch time: 19.40\n",
      "Train acc: 0.5238841862254713\n",
      "Epoch [29/100], Loss: 1.5666 - Val accuracy: 0.5169 - Epoch time: 18.04\n",
      "Train acc: 0.5239887937668334\n",
      "Epoch [30/100], Loss: 1.5662 - Val accuracy: 0.5155 - Epoch time: 18.02\n",
      "Train acc: 0.5243867833782224\n",
      "Epoch [31/100], Loss: 1.5658 - Val accuracy: 0.5166 - Epoch time: 18.09\n",
      "Train acc: 0.5241042227779915\n",
      "Epoch [32/100], Loss: 1.5655 - Val accuracy: 0.5155 - Epoch time: 17.88\n",
      "Train acc: 0.5243551205591894\n",
      "Epoch [33/100], Loss: 1.5663 - Val accuracy: 0.5152 - Epoch time: 18.05\n",
      "Train acc: 0.5243430967038605\n",
      "Epoch [34/100], Loss: 1.5654 - Val accuracy: 0.5166 - Epoch time: 17.98\n",
      "Train acc: 0.5244260613056303\n",
      "Epoch [35/100], Loss: 1.5640 - Val accuracy: 0.5155 - Epoch time: 17.89\n",
      "Epoch 00036: reducing learning rate of group 0 to 2.8750e-02.\n",
      "Train acc: 0.5245258593048608\n",
      "Epoch [36/100], Loss: 1.5650 - Val accuracy: 0.5152 - Epoch time: 18.02\n",
      "Train acc: 0.5265779306143388\n",
      "Epoch [37/100], Loss: 1.5549 - Val accuracy: 0.5190 - Epoch time: 18.19\n",
      "Train acc: 0.5273815249454918\n",
      "Epoch [38/100], Loss: 1.5531 - Val accuracy: 0.5185 - Epoch time: 18.15\n",
      "Train acc: 0.5275919424137488\n",
      "Epoch [39/100], Loss: 1.5531 - Val accuracy: 0.5173 - Epoch time: 18.16\n",
      "Train acc: 0.527492545209696\n",
      "Epoch [40/100], Loss: 1.5527 - Val accuracy: 0.5168 - Epoch time: 18.30\n",
      "Train acc: 0.5282452385532896\n",
      "Epoch [41/100], Loss: 1.5519 - Val accuracy: 0.5174 - Epoch time: 17.84\n",
      "Train acc: 0.5275310215467488\n",
      "Epoch [42/100], Loss: 1.5521 - Val accuracy: 0.5192 - Epoch time: 17.94\n",
      "Train acc: 0.5275278151853277\n",
      "Epoch [43/100], Loss: 1.5528 - Val accuracy: 0.5170 - Epoch time: 17.93\n",
      "Train acc: 0.5281346190842632\n",
      "Epoch [44/100], Loss: 1.5519 - Val accuracy: 0.5183 - Epoch time: 17.89\n",
      "Train acc: 0.52789814992946\n",
      "Epoch [45/100], Loss: 1.5512 - Val accuracy: 0.5183 - Epoch time: 18.10\n",
      "Train acc: 0.5275903392330383\n",
      "Epoch [46/100], Loss: 1.5510 - Val accuracy: 0.5180 - Epoch time: 18.03\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.4375e-02.\n",
      "Train acc: 0.528359465178915\n",
      "Epoch [47/100], Loss: 1.5508 - Val accuracy: 0.5189 - Epoch time: 17.91\n",
      "Train acc: 0.5297434109272797\n",
      "Epoch [48/100], Loss: 1.5458 - Val accuracy: 0.5197 - Epoch time: 17.92\n",
      "Train acc: 0.5301450076952674\n",
      "Epoch [49/100], Loss: 1.5452 - Val accuracy: 0.5191 - Epoch time: 17.93\n",
      "Train acc: 0.5297281807105296\n",
      "Epoch [50/100], Loss: 1.5443 - Val accuracy: 0.5194 - Epoch time: 17.73\n",
      "Train acc: 0.5301983134538926\n",
      "Epoch [51/100], Loss: 1.5448 - Val accuracy: 0.5191 - Epoch time: 17.99\n",
      "Train acc: 0.5302660478389124\n",
      "Epoch [52/100], Loss: 1.5446 - Val accuracy: 0.5198 - Epoch time: 17.90\n",
      "Train acc: 0.5298921059381814\n",
      "Epoch [53/100], Loss: 1.5444 - Val accuracy: 0.5195 - Epoch time: 18.49\n",
      "Train acc: 0.5298299826856484\n",
      "Epoch [54/100], Loss: 1.5441 - Val accuracy: 0.5194 - Epoch time: 17.96\n",
      "Train acc: 0.5300363922021291\n",
      "Epoch [55/100], Loss: 1.5451 - Val accuracy: 0.5193 - Epoch time: 18.38\n",
      "Train acc: 0.5302091349236886\n",
      "Epoch [56/100], Loss: 1.5436 - Val accuracy: 0.5190 - Epoch time: 18.23\n",
      "Train acc: 0.529818359625497\n",
      "Epoch [57/100], Loss: 1.5443 - Val accuracy: 0.5190 - Epoch time: 18.96\n",
      "Epoch 00058: reducing learning rate of group 0 to 7.1875e-03.\n",
      "Train acc: 0.5300848884186224\n",
      "Epoch [58/100], Loss: 1.5448 - Val accuracy: 0.5192 - Epoch time: 20.27\n",
      "Train acc: 0.5305758625112222\n",
      "Epoch [59/100], Loss: 1.5416 - Val accuracy: 0.5196 - Epoch time: 18.38\n",
      "Train acc: 0.5309962966525588\n",
      "Epoch [60/100], Loss: 1.5407 - Val accuracy: 0.5200 - Epoch time: 18.15\n",
      "Train acc: 0.5309205463639861\n",
      "Epoch [61/100], Loss: 1.5411 - Val accuracy: 0.5201 - Epoch time: 17.81\n",
      "Train acc: 0.5311582179043222\n",
      "Epoch [62/100], Loss: 1.5415 - Val accuracy: 0.5204 - Epoch time: 18.43\n",
      "Train acc: 0.531183468000513\n",
      "Epoch [63/100], Loss: 1.5413 - Val accuracy: 0.5198 - Epoch time: 18.83\n",
      "Train acc: 0.5311469956393484\n",
      "Epoch [64/100], Loss: 1.5412 - Val accuracy: 0.5206 - Epoch time: 19.11\n",
      "Train acc: 0.5307614306784662\n",
      "Epoch [65/100], Loss: 1.5401 - Val accuracy: 0.5190 - Epoch time: 18.60\n",
      "Train acc: 0.5306752597152752\n",
      "Epoch [66/100], Loss: 1.5411 - Val accuracy: 0.5196 - Epoch time: 18.75\n",
      "Train acc: 0.5313910799025267\n",
      "Epoch [67/100], Loss: 1.5397 - Val accuracy: 0.5200 - Epoch time: 18.64\n",
      "Train acc: 0.5309425900987559\n",
      "Epoch [68/100], Loss: 1.5410 - Val accuracy: 0.5191 - Epoch time: 18.10\n",
      "Epoch 00069: reducing learning rate of group 0 to 3.5938e-03.\n",
      "Train acc: 0.5307478036424266\n",
      "Epoch [69/100], Loss: 1.5404 - Val accuracy: 0.5197 - Epoch time: 18.01\n",
      "Train acc: 0.5311574163139668\n",
      "Epoch [70/100], Loss: 1.5399 - Val accuracy: 0.5201 - Epoch time: 17.87\n",
      "Train acc: 0.5308644350391175\n",
      "Epoch [71/100], Loss: 1.5397 - Val accuracy: 0.5200 - Epoch time: 17.89\n",
      "Train acc: 0.5319040977298961\n",
      "Epoch [72/100], Loss: 1.5385 - Val accuracy: 0.5200 - Epoch time: 18.02\n",
      "Train acc: 0.531092888290368\n",
      "Epoch [73/100], Loss: 1.5393 - Val accuracy: 0.5201 - Epoch time: 18.05\n",
      "Train acc: 0.5315513979735796\n",
      "Epoch [74/100], Loss: 1.5391 - Val accuracy: 0.5198 - Epoch time: 17.94\n",
      "Train acc: 0.5313674329870463\n",
      "Epoch [75/100], Loss: 1.5384 - Val accuracy: 0.5203 - Epoch time: 17.93\n",
      "Train acc: 0.5316656245992049\n",
      "Epoch [76/100], Loss: 1.5382 - Val accuracy: 0.5200 - Epoch time: 17.92\n",
      "Train acc: 0.5311870751571116\n",
      "Epoch [77/100], Loss: 1.5392 - Val accuracy: 0.5194 - Epoch time: 19.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.5315654258047967\n",
      "Epoch [78/100], Loss: 1.5383 - Val accuracy: 0.5204 - Epoch time: 17.99\n",
      "Train acc: 0.5311950910606643\n",
      "Epoch [79/100], Loss: 1.5383 - Val accuracy: 0.5199 - Epoch time: 18.03\n",
      "Epoch 00080: reducing learning rate of group 0 to 1.7969e-03.\n",
      "Train acc: 0.5314051077337438\n",
      "Epoch [80/100], Loss: 1.5381 - Val accuracy: 0.5197 - Epoch time: 18.02\n",
      "Train acc: 0.5319642170065411\n",
      "Epoch [81/100], Loss: 1.5366 - Val accuracy: 0.5195 - Epoch time: 18.06\n",
      "Train acc: 0.5317261446710273\n",
      "Epoch [82/100], Loss: 1.5370 - Val accuracy: 0.5202 - Epoch time: 17.91\n",
      "Train acc: 0.5316439816596126\n",
      "Epoch [83/100], Loss: 1.5375 - Val accuracy: 0.5198 - Epoch time: 17.93\n",
      "No improvement. Breaking out of the loop\n",
      "--- 1534.4202225208282 seconds --- \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words,labels in train_loader:\n",
    "        \n",
    "        if args.use_gpu:\n",
    "            window_words  = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #forward\n",
    "        outputs  = model(window_words)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt,y_pred))\n",
    "        \n",
    "        #backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader,model,gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "        \n",
    "    #save best model if metric improved\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\":epoch+1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of the loop\")\n",
    "        break\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\".format(epoch+1,args.num_epochs,np.mean(loss_epoch),tuning_metric, (time.time() - epoch_start_time)))\n",
    "    \n",
    "print(\"--- %s seconds --- \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3ebd1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(345, 100)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=345, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865deaf",
   "metadata": {},
   "source": [
    "### Generando texto 3 veces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e5ea3d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "h o l a   v e r g a <unk> <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"hola \"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk,\"caracter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d44131e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "m a d r e   e s a s   s o y <unk>   n o   q u i é n </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"madre\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk,\"caracter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7495dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "q u i e r o   q u e   e r e s   g u s t o   q u e   a h o r a   m e   d e   t u   m a d r e s   s u p e m o   s u   m a d r e <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"quier\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk,\"caracter\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2453ce",
   "metadata": {},
   "source": [
    "### Midiendo el likelihood de 5 oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a7c71c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -76.42219\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6904a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -80.721405\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"Estamos lenguaje clase en la de procesamiento de\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6fa1e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -17.178316\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"Hola a todos\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1579e9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -19.343298\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"todos a Hola\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0e9dbdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -19.64844\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"Hola todos a\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dac7f3",
   "metadata": {},
   "source": [
    "Estructuras morfologicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e0fa14ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-4.218685 madre\n",
      "-9.529079 rmeda\n",
      "-9.905755 darme\n",
      "-12.359339 demar\n",
      "-13.120632 marde\n",
      "--------------------------------------------------\n",
      "-31.143446 amder\n",
      "-31.28386 dmrae\n",
      "-32.327843 erdma\n",
      "-33.812 rmdae\n",
      "-36.56422 aedmr\n"
     ]
    }
   ],
   "source": [
    "word_list = list('madre')\n",
    "perms = [''.join(perm) for perm in permutations(word_list)]\n",
    "\n",
    "print('-' * 50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[:5] :\n",
    "    print(p, t)\n",
    "\n",
    "print('-' * 50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:] :\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b83f7b",
   "metadata": {},
   "source": [
    "### Perplejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bf427b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143.3429002637987"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(best_model, X_val, ngram_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2540b1",
   "metadata": {},
   "source": [
    "# 2. Modelo basado en palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ba0b72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos el word2vec\n",
    "word2vec_dir = \"word2vec_col_nlp_class_spring_2023/word2vec_col.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "dc41b0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_txt = pd.read_csv(word2vec_dir, sep = '\\r\\n', engine = 'python', header = None).loc[:,0].values.tolist()\n",
    "dimensions = np.array(full_txt[0].split(), dtype = int)\n",
    "full_txt = full_txt[1:]\n",
    "word2vec = dict()\n",
    "for i in range(dimensions[0]):\n",
    "    embedding = full_txt[i].split(' ')\n",
    "    word2vec[embedding[0]] = np.float_(embedding[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "53c8360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace()\n",
    "args.N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "58f54ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()\n",
    "ngram_data = NgramData(args.N,5000,tk.tokenize, embeddings_model = word2vec, nivel = \"palabra\")\n",
    "ngram_data.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "418bf708",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ngram_train, y_ngram_train = ngram_data.transform(X_train)\n",
    "X_ngram_val, y_ngram_val     = ngram_data.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ac2b3720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observations: X:(106964, 3), y: (106964,) \n",
      "Validation observations: X:(11594, 3), y: (11594,) \n"
     ]
    }
   ],
   "source": [
    "print(f'Training observations: X:{X_ngram_train.shape}, y: {y_ngram_train.shape} ')\n",
    "print(f'Validation observations: X:{X_ngram_val.shape}, y: {y_ngram_val.shape} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f3ce87a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '<s>', '<s>'],\n",
       " ['<s>', '<s>', 'lo'],\n",
       " ['<s>', 'lo', 'peor'],\n",
       " ['lo', 'peor', 'de'],\n",
       " ['peor', 'de', 'todo'],\n",
       " ['de', 'todo', 'es'],\n",
       " ['todo', 'es', 'que'],\n",
       " ['es', 'que', 'no'],\n",
       " ['que', 'no', 'me'],\n",
       " ['no', 'me', 'dan'],\n",
       " ['me', 'dan', 'por'],\n",
       " ['dan', 'por', 'un'],\n",
       " ['por', 'un', 'tiempo'],\n",
       " ['un', 'tiempo', 'y'],\n",
       " ['tiempo', 'y', 'luego'],\n",
       " ['y', 'luego', 'vuelven'],\n",
       " ['luego', 'vuelven', 'estoy'],\n",
       " ['vuelven', 'estoy', 'hasta'],\n",
       " ['estoy', 'hasta', 'la'],\n",
       " ['hasta', 'la', 'verga']]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[ngram_data.id2w[w] for w in tw] for tw in X_ngram_train[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d61c5b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lo',\n",
       " 'peor',\n",
       " 'de',\n",
       " 'todo',\n",
       " 'es',\n",
       " 'que',\n",
       " 'no',\n",
       " 'me',\n",
       " 'dan',\n",
       " 'por',\n",
       " 'un',\n",
       " 'tiempo',\n",
       " 'y',\n",
       " 'luego',\n",
       " 'vuelven',\n",
       " 'estoy',\n",
       " 'hasta',\n",
       " 'la',\n",
       " 'verga',\n",
       " 'de']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ngram_data.id2w[w] for w in y_ngram_train[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "dfe9424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Batch Size in args\n",
    "args.batch_size = 64\n",
    "\n",
    "# Num workers\n",
    "args.num_workers = 2\n",
    "\n",
    "# Train\n",
    "train_dataset = TensorDataset(torch.tensor(X_ngram_train, dtype = torch.int64),\n",
    "                              torch.tensor(y_ngram_train, dtype = torch.int64))\n",
    "train_loader  = DataLoader(train_dataset, \n",
    "                           batch_size = args.batch_size,\n",
    "                           num_workers = args.num_workers,\n",
    "                           shuffle     = True)\n",
    "# Val\n",
    "val_dataset = TensorDataset(torch.tensor(X_ngram_val, dtype = torch.int64),\n",
    "                              torch.tensor(y_ngram_val, dtype = torch.int64))\n",
    "val_loader  = DataLoader(val_dataset, \n",
    "                           batch_size  = args.batch_size,\n",
    "                           num_workers = args.num_workers,\n",
    "                           shuffle     = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b1974b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape : torch.Size([64, 3])\n",
      "y shape : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(f'X shape : {batch[0].shape}')\n",
    "print(f'y shape : {batch[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1298c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametros de la red\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.d = 100  #dimension of word embeddings\n",
    "args.d_h = 200  #dimension for hidden layer\n",
    "args.dropout = 0.1\n",
    "\n",
    "#parametros de entrenamiento\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "args.lr_patience = 20\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#saving directory\n",
    "args.savedir = \"model\"\n",
    "os.makedirs(args.savedir,exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args,ngram_data.embedding_matrix)\n",
    "\n",
    "#send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\"min\",\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "0f7d249d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.1467516447368421\n",
      "Epoch [1/100], Loss: 5.6772 - Val accuracy: 0.1906 - Epoch time: 7.84\n",
      "Train acc: 0.16359337619617223\n",
      "Epoch [2/100], Loss: 5.2004 - Val accuracy: 0.1704 - Epoch time: 7.94\n",
      "Train acc: 0.1672174043062201\n",
      "Epoch [3/100], Loss: 5.0000 - Val accuracy: 0.1615 - Epoch time: 7.70\n",
      "Train acc: 0.17227497009569379\n",
      "Epoch [4/100], Loss: 4.8459 - Val accuracy: 0.1901 - Epoch time: 7.76\n",
      "Train acc: 0.17494206040669857\n",
      "Epoch [5/100], Loss: 4.7167 - Val accuracy: 0.1671 - Epoch time: 7.53\n",
      "Train acc: 0.1782147129186603\n",
      "Epoch [6/100], Loss: 4.6066 - Val accuracy: 0.1810 - Epoch time: 7.74\n",
      "Train acc: 0.18166492224880382\n",
      "Epoch [7/100], Loss: 4.4959 - Val accuracy: 0.1963 - Epoch time: 7.60\n",
      "Train acc: 0.18428154904306218\n",
      "Epoch [8/100], Loss: 4.4010 - Val accuracy: 0.2037 - Epoch time: 8.06\n",
      "Train acc: 0.18827003588516747\n",
      "Epoch [9/100], Loss: 4.3168 - Val accuracy: 0.1829 - Epoch time: 8.56\n",
      "Train acc: 0.19038576555023923\n",
      "Epoch [10/100], Loss: 4.2416 - Val accuracy: 0.1641 - Epoch time: 9.00\n",
      "Train acc: 0.1948209479665072\n",
      "Epoch [11/100], Loss: 4.1698 - Val accuracy: 0.1305 - Epoch time: 7.89\n",
      "Train acc: 0.19842628588516747\n",
      "Epoch [12/100], Loss: 4.1039 - Val accuracy: 0.2034 - Epoch time: 7.74\n",
      "Train acc: 0.202504485645933\n",
      "Epoch [13/100], Loss: 4.0457 - Val accuracy: 0.1345 - Epoch time: 8.29\n",
      "Train acc: 0.20739383971291867\n",
      "Epoch [14/100], Loss: 3.9934 - Val accuracy: 0.1547 - Epoch time: 8.38\n",
      "Train acc: 0.2097917912679426\n",
      "Epoch [15/100], Loss: 3.9421 - Val accuracy: 0.1564 - Epoch time: 7.51\n",
      "Train acc: 0.21327190490430623\n",
      "Epoch [16/100], Loss: 3.9017 - Val accuracy: 0.1853 - Epoch time: 7.67\n",
      "Train acc: 0.2194583582535885\n",
      "Epoch [17/100], Loss: 3.8545 - Val accuracy: 0.1494 - Epoch time: 7.50\n",
      "Train acc: 0.2204694976076555\n",
      "Epoch [18/100], Loss: 3.8240 - Val accuracy: 0.1871 - Epoch time: 7.75\n",
      "Train acc: 0.22552519437799043\n",
      "Epoch [19/100], Loss: 3.7805 - Val accuracy: 0.1979 - Epoch time: 7.46\n",
      "Train acc: 0.22843151913875598\n",
      "Epoch [20/100], Loss: 3.7546 - Val accuracy: 0.1768 - Epoch time: 7.79\n",
      "Train acc: 0.2322985197368421\n",
      "Epoch [21/100], Loss: 3.7106 - Val accuracy: 0.1577 - Epoch time: 7.60\n",
      "Train acc: 0.2351768092105263\n",
      "Epoch [22/100], Loss: 3.6882 - Val accuracy: 0.1917 - Epoch time: 7.77\n",
      "Train acc: 0.2375672846889952\n",
      "Epoch [23/100], Loss: 3.6585 - Val accuracy: 0.1458 - Epoch time: 7.61\n",
      "Train acc: 0.24197630083732058\n",
      "Epoch [24/100], Loss: 3.6267 - Val accuracy: 0.2087 - Epoch time: 7.74\n",
      "Train acc: 0.244691985645933\n",
      "Epoch [25/100], Loss: 3.6099 - Val accuracy: 0.1384 - Epoch time: 7.51\n",
      "Train acc: 0.24751607356459332\n",
      "Epoch [26/100], Loss: 3.5850 - Val accuracy: 0.1799 - Epoch time: 7.81\n",
      "Train acc: 0.2500504635167464\n",
      "Epoch [27/100], Loss: 3.5582 - Val accuracy: 0.1404 - Epoch time: 7.58\n",
      "Train acc: 0.2526147577751196\n",
      "Epoch [28/100], Loss: 3.5410 - Val accuracy: 0.1540 - Epoch time: 8.19\n",
      "Train acc: 0.2539959629186603\n",
      "Epoch [29/100], Loss: 3.5223 - Val accuracy: 0.1597 - Epoch time: 7.49\n",
      "Train acc: 0.2572742224880383\n",
      "Epoch [30/100], Loss: 3.5014 - Val accuracy: 0.1680 - Epoch time: 7.64\n",
      "Train acc: 0.2591974431818182\n",
      "Epoch [31/100], Loss: 3.4784 - Val accuracy: 0.1624 - Epoch time: 7.61\n",
      "Epoch 00032: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.2627691387559809\n",
      "Epoch [32/100], Loss: 3.4609 - Val accuracy: 0.1678 - Epoch time: 7.64\n",
      "Train acc: 0.3159520783492823\n",
      "Epoch [33/100], Loss: 3.0488 - Val accuracy: 0.1849 - Epoch time: 8.09\n",
      "Train acc: 0.32308425538277513\n",
      "Epoch [34/100], Loss: 2.9715 - Val accuracy: 0.1865 - Epoch time: 7.99\n",
      "Train acc: 0.3279007177033493\n",
      "Epoch [35/100], Loss: 2.9468 - Val accuracy: 0.1879 - Epoch time: 7.72\n",
      "Train acc: 0.3295398474880383\n",
      "Epoch [36/100], Loss: 2.9355 - Val accuracy: 0.2008 - Epoch time: 7.58\n",
      "Train acc: 0.3313004635167464\n",
      "Epoch [37/100], Loss: 2.9246 - Val accuracy: 0.1930 - Epoch time: 7.70\n",
      "Train acc: 0.330928528708134\n",
      "Epoch [38/100], Loss: 2.9122 - Val accuracy: 0.1543 - Epoch time: 7.50\n",
      "Train acc: 0.3343544407894737\n",
      "Epoch [39/100], Loss: 2.9014 - Val accuracy: 0.1834 - Epoch time: 7.60\n",
      "Train acc: 0.33532446172248803\n",
      "Epoch [40/100], Loss: 2.8945 - Val accuracy: 0.1867 - Epoch time: 7.56\n",
      "Train acc: 0.33467965011961726\n",
      "Epoch [41/100], Loss: 2.8843 - Val accuracy: 0.1859 - Epoch time: 7.78\n",
      "Train acc: 0.3372103020334928\n",
      "Epoch [42/100], Loss: 2.8756 - Val accuracy: 0.1909 - Epoch time: 7.48\n",
      "Train acc: 0.3371000299043062\n",
      "Epoch [43/100], Loss: 2.8713 - Val accuracy: 0.1810 - Epoch time: 7.69\n",
      "No improvement. Breaking out of the loop\n",
      "--- 341.98745346069336 seconds --- \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words,labels in train_loader:\n",
    "        \n",
    "        if args.use_gpu:\n",
    "            window_words  = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #forward\n",
    "        outputs  = model(window_words)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt,y_pred))\n",
    "        \n",
    "        #backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader,model,gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "        \n",
    "    #save best model if metric improved\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\":epoch+1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of the loop\")\n",
    "        break\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\".format(epoch+1,args.num_epochs,np.mean(loss_epoch),tuning_metric, (time.time() - epoch_start_time)))\n",
    "    \n",
    "print(\"--- %s seconds --- \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "15138a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03ef4cd",
   "metadata": {},
   "source": [
    "### 10 palabras más cercanas a tres palabras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8bd37fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "mama 15.1532955\n",
      "abuela 15.8665285\n",
      "hermana 15.943254\n",
      "mamá 17.129944\n",
      "hija 17.444803\n",
      "suegra 17.64279\n",
      "vecina 18.390766\n",
      "papá 18.414007\n",
      "padre 19.15226\n",
      "tía 19.548462\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"madre\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5e1053ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "gato 9.573887\n",
      "perrito 12.469641\n",
      "gatito 15.573966\n",
      "enano 17.193613\n",
      "gordo 17.3278\n",
      "vecino 17.501198\n",
      "niño 17.517479\n",
      "sapo 17.797817\n",
      "macho 17.798012\n",
      "mono 17.865211\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"perro\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0a5b02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "maldito 10.946679\n",
      "malparido 14.632573\n",
      "hijodeputa 15.781651\n",
      "estupido 16.454464\n",
      "culero 16.602884\n",
      "asqueroso 16.712624\n",
      "bastardo 16.99627\n",
      "desgraciado 17.056053\n",
      "pinche 17.07916\n",
      "reverendo 17.18088\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print_closest_words(best_model.emb, ngram_data, \"puto\",10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59b0f8",
   "metadata": {},
   "source": [
    "### Generar texto a partir de tres secuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0bdb0332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "<s> <s> <s> ya lo pinches valió verga <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"<s> <s> <s>\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "31c06c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "hola a todos los putos dias del año <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"hola a todos\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "efb89d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Learned embeddings\n",
      "------------------------------\n",
      "te quiero dar en la madre <unk> 😡 </s>\n"
     ]
    }
   ],
   "source": [
    "initial_tokens = \"te quiero dar\"\n",
    "\n",
    "print(\"-\"*30)\n",
    "print(\"Learned embeddings\")\n",
    "print(\"-\"*30)\n",
    "print(generate_sentence(best_model, initial_tokens, tk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8646613",
   "metadata": {},
   "source": [
    "### 5 ejemplos y medir el likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "14348082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -36.10342\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"Estamos en la clase de procesamiento de lenguaje\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ba35b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -66.18956\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"lenguaje de Estamos la en procesamiento clase de\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "af1d1d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -11.439241\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"Chinguen su madre todos\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "197df3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -22.895464\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"su Chinguen todos madre\", ngram_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e7cc17dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood:  -14.243498\n"
     ]
    }
   ],
   "source": [
    "print(\"log_likelihood: \", log_likelihood(best_model, \"Chinguen todos su madre\", ngram_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba65e65",
   "metadata": {},
   "source": [
    "### Estructuras sintácticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "56262666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "-3.7887769 todos chinguen a su madre\n",
      "-10.807098 chinguen a su madre todos\n",
      "-13.72316 chinguen su madre a todos\n",
      "-13.897089 todos chinguen su madre a\n",
      "-14.32552 a chinguen su madre todos\n",
      "--------------------------------------------------\n",
      "-38.827095 su todos madre a chinguen\n",
      "-40.850376 a madre chinguen su todos\n",
      "-41.14011 a su todos madre chinguen\n",
      "-42.283287 madre a chinguen su todos\n",
      "-42.904263 chinguen su todos madre a\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "from random import shuffle\n",
    "\n",
    "word_list = 'chinguen a su madre todos'.split(' ')\n",
    "perms = [' '.join(perm) for perm in permutations(word_list)]\n",
    "\n",
    "print('-' * 50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[:5] :\n",
    "    print(p, t)\n",
    "\n",
    "print('-' * 50)\n",
    "for p, t in sorted([(log_likelihood(best_model, text, ngram_data), text) for text in perms], reverse = True)[-5:] :\n",
    "    print(p, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d68fc2",
   "metadata": {},
   "source": [
    "### Perplejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "9ac3f5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.62265371347404"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(best_model, X_val, ngram_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbb7b7",
   "metadata": {},
   "source": [
    "Modelo sin word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "322d6eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir = \"C:/Users/ernes/OneDrive/Documentos/Maestría-CIMAT/Segundo/NLP/Practicas/Practica5/model\"\n",
    "\n",
    "best_model_practica = NeuralLM(args)\n",
    "best_model_practica.load_state_dict(torch.load(model_dir + \"/\" + 'model_best.pt')['state_dict'])\n",
    "best_model_practica.train(False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6fc49b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112.5419541396104"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(best_model_practica, X_val, ngram_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd357e0",
   "metadata": {},
   "source": [
    "# Modelo de lenguaje que integre una conexión directa de la capa de embeddings hacía la salida,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1bd91",
   "metadata": {},
   "source": [
    "Creamos nuestro modelo de lenguaje con una conexión directa entre los embeddings y la capa de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "7562820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralLM( nn.Module ):\n",
    "    \n",
    "    def __init__(self,args, embeddings = None):\n",
    "        super(NeuralLM,self).__init__()\n",
    "        \n",
    "        self.window_size = args.N-1\n",
    "        self.embedding_dim = args.d\n",
    "        if embeddings is not None:\n",
    "            self.emb = nn.Embedding(args.vocab_size, args.d)\n",
    "            for i in range(embeddings.shape[0]):\n",
    "                for j in range(embeddings.shape[1]):\n",
    "                    self.emb.weight.data[i][j] = embeddings[i][j]\n",
    "                    \n",
    "        else :\n",
    "            self.emb   = nn.Embedding(args.vocab_size, args.d)\n",
    "        \n",
    "        self.fc1 = nn.Linear(args.d*(args.N-1),args.d_h)\n",
    "        self.drop1 = nn.Dropout(p=args.dropout)\n",
    "        self.fc2 = nn.Linear(args.d_h,args.vocab_size,bias=False)\n",
    "        self.fc3   = nn.Linear(args.d*(args.N-1), args.vocab_size, bias = False)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(-1,self.window_size*self.embedding_dim)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop1(h)\n",
    "        return self.fc2(h) + self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400cab14",
   "metadata": {},
   "source": [
    "Como ya tenemos los datos guardados en el punto anterior los vamos a reutilizar y solo crear una nueva instancia del modelo modificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "26b4afd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametros de la red\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.d = 100  #dimension of word embeddings\n",
    "args.d_h = 200  #dimension for hidden layer\n",
    "args.dropout = 0.1\n",
    "\n",
    "#parametros de entrenamiento\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "args.lr_patience = 20\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#saving directory\n",
    "args.savedir = \"model\"\n",
    "os.makedirs(args.savedir,exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args,ngram_data.embedding_matrix)\n",
    "\n",
    "#send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\"min\",\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "fa2047c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.111565490430622\n",
      "Epoch [1/100], Loss: 8.6190 - Val accuracy: 0.1620 - Epoch time: 8.61\n",
      "Train acc: 0.12478693181818183\n",
      "Epoch [2/100], Loss: 7.2368 - Val accuracy: 0.1183 - Epoch time: 8.37\n",
      "Train acc: 0.13623280502392343\n",
      "Epoch [3/100], Loss: 6.5823 - Val accuracy: 0.1131 - Epoch time: 8.15\n",
      "Train acc: 0.15503887559808613\n",
      "Epoch [4/100], Loss: 6.0690 - Val accuracy: 0.1178 - Epoch time: 8.17\n",
      "Train acc: 0.17028072667464117\n",
      "Epoch [5/100], Loss: 5.7458 - Val accuracy: 0.1063 - Epoch time: 8.46\n",
      "Train acc: 0.1856870514354067\n",
      "Epoch [6/100], Loss: 5.4289 - Val accuracy: 0.1360 - Epoch time: 8.12\n",
      "Train acc: 0.2003102571770335\n",
      "Epoch [7/100], Loss: 5.1875 - Val accuracy: 0.1307 - Epoch time: 8.21\n",
      "Train acc: 0.21177295155502393\n",
      "Epoch [8/100], Loss: 5.0138 - Val accuracy: 0.1164 - Epoch time: 8.29\n",
      "Train acc: 0.22142269736842105\n",
      "Epoch [9/100], Loss: 4.8662 - Val accuracy: 0.1193 - Epoch time: 8.02\n",
      "Train acc: 0.2347357206937799\n",
      "Epoch [10/100], Loss: 4.7004 - Val accuracy: 0.1132 - Epoch time: 8.18\n",
      "Train acc: 0.2433724581339713\n",
      "Epoch [11/100], Loss: 4.5640 - Val accuracy: 0.1252 - Epoch time: 8.11\n",
      "Train acc: 0.25251943779904307\n",
      "Epoch [12/100], Loss: 4.4655 - Val accuracy: 0.1324 - Epoch time: 8.28\n",
      "Train acc: 0.26012447667464117\n",
      "Epoch [13/100], Loss: 4.3546 - Val accuracy: 0.1070 - Epoch time: 7.95\n",
      "Train acc: 0.2699966357655502\n",
      "Epoch [14/100], Loss: 4.2688 - Val accuracy: 0.1479 - Epoch time: 8.27\n",
      "Train acc: 0.27428042763157895\n",
      "Epoch [15/100], Loss: 4.1748 - Val accuracy: 0.1431 - Epoch time: 8.08\n",
      "Train acc: 0.28391895933014355\n",
      "Epoch [16/100], Loss: 4.1030 - Val accuracy: 0.1459 - Epoch time: 8.30\n",
      "Train acc: 0.2891895933014354\n",
      "Epoch [17/100], Loss: 4.0352 - Val accuracy: 0.1345 - Epoch time: 8.07\n",
      "Train acc: 0.2945742374401914\n",
      "Epoch [18/100], Loss: 3.9786 - Val accuracy: 0.1499 - Epoch time: 9.10\n",
      "Train acc: 0.3016839862440191\n",
      "Epoch [19/100], Loss: 3.8938 - Val accuracy: 0.1596 - Epoch time: 8.73\n",
      "Train acc: 0.3077601674641149\n",
      "Epoch [20/100], Loss: 3.8320 - Val accuracy: 0.1485 - Epoch time: 8.35\n",
      "Train acc: 0.3127111991626794\n",
      "Epoch [21/100], Loss: 3.7851 - Val accuracy: 0.1719 - Epoch time: 8.17\n",
      "Train acc: 0.31799304724880384\n",
      "Epoch [22/100], Loss: 3.7255 - Val accuracy: 0.1953 - Epoch time: 8.39\n",
      "Train acc: 0.3218488337320574\n",
      "Epoch [23/100], Loss: 3.6863 - Val accuracy: 0.1299 - Epoch time: 8.07\n",
      "Train acc: 0.3256934061004785\n",
      "Epoch [24/100], Loss: 3.6437 - Val accuracy: 0.1484 - Epoch time: 8.18\n",
      "Train acc: 0.3325097188995215\n",
      "Epoch [25/100], Loss: 3.5805 - Val accuracy: 0.1446 - Epoch time: 8.22\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.3362982206937799\n",
      "Epoch [26/100], Loss: 3.5575 - Val accuracy: 0.1764 - Epoch time: 8.21\n",
      "Train acc: 0.40816387559808615\n",
      "Epoch [27/100], Loss: 2.7193 - Val accuracy: 0.1910 - Epoch time: 8.04\n",
      "Train acc: 0.4212357954545454\n",
      "Epoch [28/100], Loss: 2.5693 - Val accuracy: 0.1609 - Epoch time: 8.09\n",
      "Train acc: 0.42588965311004784\n",
      "Epoch [29/100], Loss: 2.5320 - Val accuracy: 0.1634 - Epoch time: 8.13\n",
      "Train acc: 0.4282577003588517\n",
      "Epoch [30/100], Loss: 2.5099 - Val accuracy: 0.1760 - Epoch time: 8.24\n",
      "Train acc: 0.43173594497607654\n",
      "Epoch [31/100], Loss: 2.4928 - Val accuracy: 0.1823 - Epoch time: 8.08\n",
      "Train acc: 0.4321190938995215\n",
      "Epoch [32/100], Loss: 2.4814 - Val accuracy: 0.1709 - Epoch time: 8.34\n",
      "Train acc: 0.43456937799043066\n",
      "Epoch [33/100], Loss: 2.4688 - Val accuracy: 0.1584 - Epoch time: 8.21\n",
      "Train acc: 0.43627205442583733\n",
      "Epoch [34/100], Loss: 2.4496 - Val accuracy: 0.1749 - Epoch time: 8.17\n",
      "Train acc: 0.43756167763157894\n",
      "Epoch [35/100], Loss: 2.4465 - Val accuracy: 0.1779 - Epoch time: 8.12\n",
      "Train acc: 0.43856160287081336\n",
      "Epoch [36/100], Loss: 2.4336 - Val accuracy: 0.1642 - Epoch time: 8.26\n",
      "Train acc: 0.44173706638755983\n",
      "Epoch [37/100], Loss: 2.4183 - Val accuracy: 0.1661 - Epoch time: 7.96\n",
      "Train acc: 0.4431425687799043\n",
      "Epoch [38/100], Loss: 2.4138 - Val accuracy: 0.1401 - Epoch time: 8.21\n",
      "Train acc: 0.4409875897129186\n",
      "Epoch [39/100], Loss: 2.4132 - Val accuracy: 0.1763 - Epoch time: 8.05\n",
      "Train acc: 0.443964937200957\n",
      "Epoch [40/100], Loss: 2.3945 - Val accuracy: 0.1928 - Epoch time: 8.14\n",
      "Train acc: 0.4455106160287081\n",
      "Epoch [41/100], Loss: 2.3902 - Val accuracy: 0.1735 - Epoch time: 8.04\n",
      "No improvement. Breaking out of the loop\n",
      "--- 345.19643545150757 seconds --- \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words,labels in train_loader:\n",
    "        \n",
    "        if args.use_gpu:\n",
    "            window_words  = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #forward\n",
    "        outputs  = model(window_words)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt,y_pred))\n",
    "        \n",
    "        #backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader,model,gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "        \n",
    "    #save best model if metric improved\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\":epoch+1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of the loop\")\n",
    "        break\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\".format(epoch+1,args.num_epochs,np.mean(loss_epoch),tuning_metric, (time.time() - epoch_start_time)))\n",
    "    \n",
    "print(\"--- %s seconds --- \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "105055f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralLM(\n",
       "  (emb): Embedding(5000, 100)\n",
       "  (fc1): Linear(in_features=300, out_features=200, bias=True)\n",
       "  (drop1): Dropout(p=0.1, inplace=False)\n",
       "  (fc2): Linear(in_features=200, out_features=5000, bias=False)\n",
       "  (fc3): Linear(in_features=300, out_features=5000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = NeuralLM(args)\n",
    "best_model.load_state_dict(torch.load(\"model/model_best.pt\")[\"state_dict\"])\n",
    "best_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e8540f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164.3523234577922"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(best_model, X_val, ngram_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63c840",
   "metadata": {},
   "source": [
    "Ahora realizamos el modelo sin embeddings preentrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9a01baa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hiperparametros de la red\n",
    "args.vocab_size = ngram_data.get_vocab_size()\n",
    "args.d = 100  #dimension of word embeddings\n",
    "args.d_h = 200  #dimension for hidden layer\n",
    "args.dropout = 0.1\n",
    "\n",
    "#parametros de entrenamiento\n",
    "args.lr = 2.3e-1\n",
    "args.num_epochs = 100\n",
    "args.patience = 20\n",
    "\n",
    "args.lr_patience = 20\n",
    "args.lr_factor = 0.5\n",
    "\n",
    "#saving directory\n",
    "args.savedir = \"model\"\n",
    "os.makedirs(args.savedir,exist_ok=True)\n",
    "\n",
    "#Create model\n",
    "model = NeuralLM(args)\n",
    "\n",
    "#send to GPU\n",
    "args.use_gpu = torch.cuda.is_available()\n",
    "if args.use_gpu:\n",
    "    model.cuda()\n",
    "    \n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\"min\",\n",
    "                patience=args.lr_patience,\n",
    "                verbose=True,\n",
    "                factor=args.lr_factor\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4c029721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.15254373504784688\n",
      "Epoch [1/100], Loss: 5.5821 - Val accuracy: 0.1674 - Epoch time: 8.37\n",
      "Train acc: 0.17065453050239235\n",
      "Epoch [2/100], Loss: 4.8795 - Val accuracy: 0.1902 - Epoch time: 8.81\n",
      "Train acc: 0.18176584928229667\n",
      "Epoch [3/100], Loss: 4.5314 - Val accuracy: 0.1885 - Epoch time: 8.48\n",
      "Train acc: 0.19350515849282296\n",
      "Epoch [4/100], Loss: 4.2593 - Val accuracy: 0.1629 - Epoch time: 8.36\n",
      "Train acc: 0.2041193181818182\n",
      "Epoch [5/100], Loss: 4.0312 - Val accuracy: 0.1994 - Epoch time: 8.56\n",
      "Train acc: 0.21449237440191388\n",
      "Epoch [6/100], Loss: 3.8340 - Val accuracy: 0.1976 - Epoch time: 8.13\n",
      "Train acc: 0.22802968002392343\n",
      "Epoch [7/100], Loss: 3.6570 - Val accuracy: 0.1871 - Epoch time: 8.22\n",
      "Train acc: 0.24819452751196172\n",
      "Epoch [8/100], Loss: 3.5011 - Val accuracy: 0.1734 - Epoch time: 8.59\n",
      "Train acc: 0.2670305023923445\n",
      "Epoch [9/100], Loss: 3.3667 - Val accuracy: 0.2090 - Epoch time: 8.51\n",
      "Train acc: 0.2828517494019139\n",
      "Epoch [10/100], Loss: 3.2539 - Val accuracy: 0.1954 - Epoch time: 8.39\n",
      "Train acc: 0.3014111094497608\n",
      "Epoch [11/100], Loss: 3.1524 - Val accuracy: 0.1829 - Epoch time: 8.46\n",
      "Train acc: 0.31259905801435406\n",
      "Epoch [12/100], Loss: 3.0678 - Val accuracy: 0.1512 - Epoch time: 8.16\n",
      "Train acc: 0.32602609150717704\n",
      "Epoch [13/100], Loss: 2.9939 - Val accuracy: 0.1969 - Epoch time: 8.47\n",
      "Train acc: 0.3373523474880383\n",
      "Epoch [14/100], Loss: 2.9269 - Val accuracy: 0.1860 - Epoch time: 8.49\n",
      "Train acc: 0.34727683911483254\n",
      "Epoch [15/100], Loss: 2.8666 - Val accuracy: 0.2158 - Epoch time: 8.59\n",
      "Train acc: 0.35484262858851673\n",
      "Epoch [16/100], Loss: 2.8180 - Val accuracy: 0.2127 - Epoch time: 8.56\n",
      "Train acc: 0.3637148624401914\n",
      "Epoch [17/100], Loss: 2.7669 - Val accuracy: 0.1952 - Epoch time: 8.38\n",
      "Train acc: 0.37098534688995216\n",
      "Epoch [18/100], Loss: 2.7268 - Val accuracy: 0.1722 - Epoch time: 8.09\n",
      "Train acc: 0.37901839114832536\n",
      "Epoch [19/100], Loss: 2.6867 - Val accuracy: 0.1935 - Epoch time: 8.33\n",
      "Train acc: 0.38347039473684214\n",
      "Epoch [20/100], Loss: 2.6507 - Val accuracy: 0.1576 - Epoch time: 7.84\n",
      "Train acc: 0.3900213068181818\n",
      "Epoch [21/100], Loss: 2.6128 - Val accuracy: 0.1584 - Epoch time: 8.22\n",
      "Train acc: 0.39457236842105264\n",
      "Epoch [22/100], Loss: 2.5827 - Val accuracy: 0.1636 - Epoch time: 7.98\n",
      "Train acc: 0.39910660885167465\n",
      "Epoch [23/100], Loss: 2.5524 - Val accuracy: 0.1802 - Epoch time: 8.14\n",
      "Train acc: 0.4051846590909091\n",
      "Epoch [24/100], Loss: 2.5255 - Val accuracy: 0.1825 - Epoch time: 7.88\n",
      "Train acc: 0.40811528110047846\n",
      "Epoch [25/100], Loss: 2.4964 - Val accuracy: 0.1917 - Epoch time: 8.35\n",
      "Train acc: 0.4140400717703349\n",
      "Epoch [26/100], Loss: 2.4714 - Val accuracy: 0.1931 - Epoch time: 7.99\n",
      "Train acc: 0.4165127840909091\n",
      "Epoch [27/100], Loss: 2.4466 - Val accuracy: 0.1710 - Epoch time: 8.16\n",
      "Train acc: 0.4209760017942584\n",
      "Epoch [28/100], Loss: 2.4211 - Val accuracy: 0.1870 - Epoch time: 8.08\n",
      "Train acc: 0.4248149671052631\n",
      "Epoch [29/100], Loss: 2.4008 - Val accuracy: 0.1821 - Epoch time: 8.27\n",
      "Train acc: 0.4292800538277512\n",
      "Epoch [30/100], Loss: 2.3744 - Val accuracy: 0.1986 - Epoch time: 8.13\n",
      "Train acc: 0.43174155203349285\n",
      "Epoch [31/100], Loss: 2.3600 - Val accuracy: 0.1753 - Epoch time: 8.18\n",
      "Train acc: 0.4346946022727273\n",
      "Epoch [32/100], Loss: 2.3351 - Val accuracy: 0.1644 - Epoch time: 8.13\n",
      "Epoch 00033: reducing learning rate of group 0 to 1.1500e-01.\n",
      "Train acc: 0.43772802033492825\n",
      "Epoch [33/100], Loss: 2.3199 - Val accuracy: 0.1800 - Epoch time: 8.04\n",
      "Train acc: 0.4862122458133971\n",
      "Epoch [34/100], Loss: 2.0838 - Val accuracy: 0.1944 - Epoch time: 8.17\n",
      "No improvement. Breaking out of the loop\n",
      "--- 289.56698203086853 seconds --- \n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "best_metric = 0\n",
    "metric_history = []\n",
    "train_metric_history = []\n",
    "\n",
    "\n",
    "for epoch in range(args.num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    loss_epoch = []\n",
    "    training_metric = []\n",
    "    model.train()\n",
    "    \n",
    "    for window_words,labels in train_loader:\n",
    "        \n",
    "        if args.use_gpu:\n",
    "            window_words  = window_words.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #forward\n",
    "        outputs  = model(window_words)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss_epoch.append(loss.item())\n",
    "        \n",
    "        #get training metrics\n",
    "        y_pred = get_preds(outputs)\n",
    "        tgt = labels.cpu().numpy()\n",
    "        training_metric.append(accuracy_score(tgt,y_pred))\n",
    "        \n",
    "        #backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    #get metric in training dataset\n",
    "    mean_epoch_metric = np.mean(training_metric)\n",
    "    train_metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #get metric in validation dataset\n",
    "    model.eval()\n",
    "    tuning_metric = model_eval(val_loader,model,gpu=args.use_gpu)\n",
    "    metric_history.append(mean_epoch_metric)\n",
    "    \n",
    "    #update scheduler\n",
    "    scheduler.step(tuning_metric)\n",
    "    \n",
    "    #check for metric improvement\n",
    "    is_improvement = tuning_metric > best_metric\n",
    "    if is_improvement:\n",
    "        best_metric = tuning_metric\n",
    "        n_no_improve = 0\n",
    "    else:\n",
    "        n_no_improve += 1\n",
    "        \n",
    "    #save best model if metric improved\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\":epoch+1,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "            \"best_metric\": best_metric,\n",
    "        },\n",
    "        is_improvement,\n",
    "        args.savedir,\n",
    "    )\n",
    "    \n",
    "    #early stopping\n",
    "    if n_no_improve >= args.patience:\n",
    "        print(\"No improvement. Breaking out of the loop\")\n",
    "        break\n",
    "        \n",
    "    print(\"Train acc: {}\".format(mean_epoch_metric))\n",
    "    print(\"Epoch [{}/{}], Loss: {:.4f} - Val accuracy: {:.4f} - Epoch time: {:.2f}\".format(epoch+1,args.num_epochs,np.mean(loss_epoch),tuning_metric, (time.time() - epoch_start_time)))\n",
    "    \n",
    "print(\"--- %s seconds --- \" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f34399",
   "metadata": {},
   "source": [
    "A conitnuación presentamos en una tabla la recopilación de todos los datos de los 4 modelos a comparar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a8dc1ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164.3523234577922"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(best_model, X_val, ngram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "24007bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train acc</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Val acc</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Time</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo 1 con Embeddings preentrenados</th>\n",
       "      <td>0.337100</td>\n",
       "      <td>2.8713</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>43.0</td>\n",
       "      <td>341.9800</td>\n",
       "      <td>113.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modelo con conexión</th>\n",
       "      <td>0.445500</td>\n",
       "      <td>2.3900</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>41.0</td>\n",
       "      <td>345.1900</td>\n",
       "      <td>164.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Modelo con conexión y Embeddings preentrenados</th>\n",
       "      <td>0.486212</td>\n",
       "      <td>2.0838</td>\n",
       "      <td>0.1944</td>\n",
       "      <td>34.0</td>\n",
       "      <td>289.5669</td>\n",
       "      <td>164.352323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Train acc    Loss  Val acc  \\\n",
       "Modelo 1 con Embeddings preentrenados            0.337100  2.8713   0.1810   \n",
       "Modelo con conexión                              0.445500  2.3900   0.1735   \n",
       "Modelo con conexión y Embeddings preentrenados   0.486212  2.0838   0.1944   \n",
       "\n",
       "                                                Epochs      Time  Perplexity  \n",
       "Modelo 1 con Embeddings preentrenados             43.0  341.9800  113.622600  \n",
       "Modelo con conexión                               41.0  345.1900  164.352300  \n",
       "Modelo con conexión y Embeddings preentrenados    34.0  289.5669  164.352323  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = {'Modelo 1 con Embeddings preentrenados': {'Train acc': 0.3371, \"Loss\":2.8713, 'Val acc': 0.1810,'Epochs': 43, 'Time': 341.98, 'Perplexity': 113.6226}, \n",
    "            'Modelo con conexión': {'Train acc': 0.4455, \"Loss\": 2.39, 'Val acc': 0.1735,'Epochs': 41, 'Time': 345.19, 'Perplexity': 164.3523},\n",
    "            'Modelo con conexión y Embeddings preentrenados': {'Train acc': 0.4862122458133971, \"Loss\": 2.0838, 'Val acc': 0.1944,'Epochs': 34, 'Time': 289.5669, 'Perplexity' : 164.3523234577922}\n",
    "\n",
    "            }\n",
    "pd.DataFrame(training).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1346e319",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c3ff1",
   "metadata": {},
   "source": [
    "Notemos que al modificar el modelo agregando la conexión directa entre los embeddings y la salida obtenemos una mejor considerable en el accuracy del conjunto de entrenamiento. Pero con respecto al accuracy del concjunto de validación si bien aumenta al añadir los embeddings preentrenados esto no es por mucho, nada significativo. Por otro lado la perplejidad de los modelos con la conexión que añadimos si presentamos un mayor valor de perplejidad, muy parecido entre usar embeddings preentrenados o no. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
