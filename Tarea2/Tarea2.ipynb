{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bddba19",
   "metadata": {},
   "source": [
    "# Tarea 2 \n",
    "\n",
    "# Procesamiento de Lenguaje Natural\n",
    "\n",
    "# Ernesto Antonio Reyes Ramírez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b8015",
   "metadata": {},
   "source": [
    "# 1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4854625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 12:00:35.647516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-13 12:00:38.374465: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-13 12:00:38.374548: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-13 12:00:43.912120: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-13 12:00:43.912426: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-13 12:00:43.912455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37944acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_file(path_corpus,path_truth):\n",
    "    tr_text = []\n",
    "    tr_y = []\n",
    "    \n",
    "    with open(path_corpus,\"r\") as f_corpus, open(path_truth,\"r\") as f_truth:\n",
    "        for twitt in f_corpus:\n",
    "            tr_text += [twitt]\n",
    "        for label in f_truth:\n",
    "            tr_y += [label]\n",
    "    return tr_text,tr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "473b9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text,tr_y = get_texts_from_file(\"./mex_train.txt\", \"./mex_train_labels.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101503f9",
   "metadata": {},
   "source": [
    "Construcción simple del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0358a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83a541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "corpus_palabras = []\n",
    "\n",
    "for doc in tr_text:\n",
    "    corpus_palabras += tokenizer.tokenize(doc)\n",
    "    \n",
    "fdist = nltk.FreqDist(corpus_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e7b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortFreqDict(freqdict):\n",
    "    aux = [(freqdict[key],key) for key in freqdict]\n",
    "    aux.sort()\n",
    "    aux.reverse()\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c8c9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = sortFreqDict(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d678e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = V[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c4506f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indices = dict()\n",
    "cont = 0 \n",
    "\n",
    "for weight, word in V:\n",
    "    dict_indices[word] = cont\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2a210",
   "metadata": {},
   "source": [
    "Términos de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51948033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 397, 1: 219})\n"
     ]
    }
   ],
   "source": [
    "val_text,val_y = get_texts_from_file(\"./mex_val.txt\",\"./mex_val_labels.txt\")\n",
    "val_y = list(map(int,val_y))\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d6ef24",
   "metadata": {},
   "source": [
    "# 2. Bolsas de palabras, Bigramas y Emociones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad84c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos las librerias necesarias para crear y entrenar un modelo de svm\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "tr_y = list(map(int,tr_y))\n",
    "\n",
    "parameters = {'C':[.05,.12,.25,.5,1,2,4]}\n",
    "\n",
    "#creamos nuestro modelo\n",
    "svr = svm.LinearSVC(class_weight=\"balanced\")\n",
    "grid = GridSearchCV(estimator=svr,param_grid = parameters, n_jobs=4,scoring=\"f1_macro\", cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68f00be",
   "metadata": {},
   "source": [
    " # BoW con pesado binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc35a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#función que crea la BoW para un pesado binario\n",
    "def build_bow_bi(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(V)),dtype=int)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc,dict_indices[word]] = 1\n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10943f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tr_bi = build_bow_bi(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f82d1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_tr_bi,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84b66735",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_bi = build_bow_bi(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7866254c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[329  68]\n",
      " [ 47 172]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.85       397\n",
      "           1       0.72      0.79      0.75       219\n",
      "\n",
      "    accuracy                           0.81       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.81      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_bi)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91750f",
   "metadata": {},
   "source": [
    "# BoW  pesado con frecuencia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "769b014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW con pesado frecuencia\n",
    "\n",
    "def build_bow_fq(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(V)),dtype=int)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc,dict_indices[word]] = fdist_doc[word]\n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9736d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tr_fq = build_bow_fq(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cb3aca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_tr_fq,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ee415cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_fq = build_bow_fq(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f995a549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334  63]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86       397\n",
      "           1       0.73      0.78      0.75       219\n",
      "\n",
      "    accuracy                           0.82       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.82      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_fq)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8693fa42",
   "metadata": {},
   "source": [
    "# BoW con pesado tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4bee67",
   "metadata": {},
   "source": [
    "Lo que haremos será primero calcular el número de veces que ocurre cada una de las 5000 palabras y\n",
    "guardarlo en un diccionario, para que al momento de realizar el pesado tfidf sea más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e06317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = []\n",
    "for tr in tr_text:\n",
    "    vec_df.append(tokenizer.tokenize(tr))\n",
    "    \n",
    "df = dict()\n",
    "for aux, word in V:\n",
    "    cont = 0\n",
    "    for tr in vec_df:\n",
    "        if word in tr:\n",
    "            cont += 1\n",
    "    df[word] = cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e42d2e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_tfidf(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(V)),dtype=float)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc,dict_indices[word]] = fdist_doc[word]*np.log(len(tr_text)/df[word])\n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "85201844",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tr_tfidf = build_bow_tfidf(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "23e93bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_tr_tfidf,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a48e1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_tfidf = build_bow_tfidf(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "17610308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[368  29]\n",
      " [161  58]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.79       397\n",
      "           1       0.67      0.26      0.38       219\n",
      "\n",
      "    accuracy                           0.69       616\n",
      "   macro avg       0.68      0.60      0.59       616\n",
      "weighted avg       0.69      0.69      0.65       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_tfidf)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa93d3",
   "metadata": {},
   "source": [
    "# BoW con pesado binario normalizado l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "faf665e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función que crea la BoW para un pesado binario\n",
    "def build_bow_bn(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(V)),dtype=float)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc,dict_indices[word]] = 1\n",
    "        cont_doc += 1\n",
    "        \n",
    "    #Calculamos las normas\n",
    "    new_weight = []\n",
    "    for i in range(len(tr_text)):\n",
    "        tmp = 0\n",
    "        for j in range(len(V)):\n",
    "            tmp = tmp+BOW[i,j]**2\n",
    "        tmp = np.sqrt(tmp)\n",
    "        new_weight.append(tmp)\n",
    "        \n",
    "    #Actualizamos los pesos\n",
    "    for i in range(len(tr_text)):\n",
    "        for j in range(len(V)):\n",
    "            BOW[i,j] = BOW[i,j]/new_weight[i] \n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "afcc49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tr_bn = build_bow_bn(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dfa1c031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_tr_bn,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f1993930",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_bn = build_bow_bn(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d9bf4b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[323  74]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       397\n",
      "           1       0.70      0.78      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.79      0.79       616\n",
      "weighted avg       0.81      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_bn)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a13ca41",
   "metadata": {},
   "source": [
    "# BoW pesado con frecuencia normalizado l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "123c89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW pesado con frecuencia normalizado l2\n",
    "\n",
    "def build_bow_fqn(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(V)),dtype=float)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc,dict_indices[word]] = fdist_doc[word]\n",
    "        cont_doc += 1\n",
    "    \n",
    "    #Calculamos las normas\n",
    "    new_weight = []\n",
    "    for i in range(len(tr_text)):\n",
    "        tmp = 0\n",
    "        for j in range(len(V)):\n",
    "            tmp = tmp+BOW[i,j]**2\n",
    "        tmp = np.sqrt(tmp)\n",
    "        new_weight.append(tmp)\n",
    "        \n",
    "    #Actualizamos los pesos\n",
    "    for i in range(len(tr_text)):\n",
    "        for j in range(len(V)):\n",
    "            BOW[i,j] = BOW[i,j]/new_weight[i] \n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bebc8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tr_fqn = build_bow_fqn(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "23c88b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_tr_fqn,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "050ab20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_fqn = build_bow_fqn(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "42711798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[321  76]\n",
      " [ 50 169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84       397\n",
      "           1       0.69      0.77      0.73       219\n",
      "\n",
      "    accuracy                           0.80       616\n",
      "   macro avg       0.78      0.79      0.78       616\n",
      "weighted avg       0.80      0.80      0.80       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_fqn)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9286ef",
   "metadata": {},
   "source": [
    "# BoW con pesado tfidf normalizado l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "05695edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW pesado tdidf normalizado l2\n",
    "\n",
    "def build_bow_tfidfn(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(V)),dtype=float)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                BOW[cont_doc,dict_indices[word]] = fdist_doc[word]*np.log(len(tr_text)/df[word])\n",
    "        cont_doc += 1\n",
    "        \n",
    "    #Calculamos las normas\n",
    "    new_weight = []\n",
    "    for i in range(len(tr_text)):\n",
    "        tmp = 0\n",
    "        for j in range(len(V)):\n",
    "            tmp = tmp+BOW[i,j]**2\n",
    "        tmp = np.sqrt(tmp)\n",
    "        new_weight.append(tmp)\n",
    "        \n",
    "    #Actualizamos los pesos\n",
    "    for i in range(len(tr_text)):\n",
    "        for j in range(len(V)):\n",
    "            BOW[i,j] = BOW[i,j]/new_weight[i] \n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "80f1ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_tr_tfidfn = build_bow_tfidfn(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8311c30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_tr_tfidfn,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f7b6eb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_tfidfn = build_bow_tfidfn(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "64fe0c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[378  19]\n",
      " [150  69]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82       397\n",
      "           1       0.78      0.32      0.45       219\n",
      "\n",
      "    accuracy                           0.73       616\n",
      "   macro avg       0.75      0.63      0.63       616\n",
      "weighted avg       0.74      0.73      0.69       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_tfidfn)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32301ab2",
   "metadata": {},
   "source": [
    "# Tabla comparativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728047d4",
   "metadata": {},
   "source": [
    "|  Pesado  | F1 score de la etiqueta 1| Accuracy  | macro avg\n",
    "|-----------|----------|------------- | -----------\n",
    "| Binario |  0.75  |  0.81    | 0.80\n",
    "| Frecuencia |  0.75  |  0.82 |  0.80\n",
    "| TfIdf |  0.38  |  0.69   | 0.59\n",
    "| Binario Norma l2  | 0.73  |  0.80   | 0.79\n",
    "| Frecuencia Norma l2  | 0.73  |   0.80| 0.78\n",
    "| TfIdf Norma l2 |  0.45  |  0.73|   0.63"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28538d",
   "metadata": {},
   "source": [
    "De las configuraciones anteriores elija la mejor y evalúela con más y menos términos\n",
    "(e.g., 1000 y 7000). Ponga una tabla dónde compare las tres configuraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c8bbd5",
   "metadata": {},
   "source": [
    "Sol: Vamos a tomar como mejor modelo el que utiliza el pesado con frecuencia y trabajar sobre él. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "01979b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7000 palabras más frecuentes\n",
    "W = sortFreqDict(fdist)\n",
    "A = W[:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7af6d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indices1 = dict()\n",
    "cont = 0 \n",
    "\n",
    "for weight, word in A:\n",
    "    dict_indices1[word] = cont\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ea6fb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_fq_7000 = build_bow_fq(tr_text,A,dict_indices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8c4d557a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_fq_7000,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c53fcf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_fq_7000 = build_bow_fq(val_text,A,dict_indices1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bfe85f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[334  63]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86       397\n",
      "           1       0.73      0.78      0.75       219\n",
      "\n",
      "    accuracy                           0.82       616\n",
      "   macro avg       0.80      0.81      0.80       616\n",
      "weighted avg       0.82      0.82      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_fq_7000)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "074f7f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1000 más usados \n",
    "E = W[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "dedc9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_indices2 = dict()\n",
    "cont = 0 \n",
    "\n",
    "for weight, word in E:\n",
    "    dict_indices2[word] = cont\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "9b136bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_fq_1000 = build_bow_fq(tr_text,E,dict_indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e5aeaa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_fq_1000,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aada4076",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_fq_1000 = build_bow_fq(val_text,E,dict_indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e3129a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[336  61]\n",
      " [ 49 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86       397\n",
      "           1       0.74      0.78      0.76       219\n",
      "\n",
      "    accuracy                           0.82       616\n",
      "   macro avg       0.80      0.81      0.81       616\n",
      "weighted avg       0.82      0.82      0.82       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_fq_1000)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f914cb",
   "metadata": {},
   "source": [
    "# Tabla comparativa entre distintas poblaciones del pesado con frecuencia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c9ec8",
   "metadata": {},
   "source": [
    "|  No. de términos  | F1 score de la etiqueta 1| Accuracy  | macro avg\n",
    "|-----------|----------|------------- | -----------\n",
    "| 7000 |  0.75  |  0.81    | 0.80\n",
    "| 5000 |  0.75  |  0.82 |  0.80\n",
    "| 1000 |  0.38  |  0.69   | 0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061bea1c",
   "metadata": {},
   "source": [
    "# Bolsa de Emociones con EmoLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "952a2e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "with open(\"./Spanish-NRC-EmoLex.txt\",\"r\") as f:\n",
    "        for emotion in f:\n",
    "            table += [tokenizer.tokenize(emotion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b02582f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emotions = ['anger','anticipation','disgust','fear','joy','negative','positive','sadness','surprise','trust',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "72a1cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6fe3c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [p for p in table if len(p)==12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c3cd9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valores = dict()\n",
    "\n",
    "for word in table:\n",
    "    aux = []\n",
    "    for i in range(1,11):\n",
    "        aux.append(int(word[i]))\n",
    "    valores[word[11]] = aux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "ad3e8118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_boe(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(emotions)),dtype=float)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                if word in valores:\n",
    "                    for i in range(0,10):\n",
    "                        BOW[cont_doc,i] = BOW[cont_doc,i] + fdist_doc[word]*valores[word][i]\n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a3c37d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_fq = build_boe(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "14845254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 2., 0., ..., 1., 0., 0.],\n",
       "       [0., 2., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 1., 0., 2.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aqui tenemos la bolsa de emociones\n",
    "BOE_fq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7d2c91",
   "metadata": {},
   "source": [
    "# Evalúa tu BoE clasificando con SVM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c9b3f46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pesado con frecuencia \n",
    "grid.fit(BOE_fq,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "086f5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_val_fq = build_boe(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a946b590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[197 200]\n",
      " [ 77 142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.50      0.59       397\n",
      "           1       0.42      0.65      0.51       219\n",
      "\n",
      "    accuracy                           0.55       616\n",
      "   macro avg       0.57      0.57      0.55       616\n",
      "weighted avg       0.61      0.55      0.56       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOE_val_fq)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "9a00008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pesado binario\n",
    "def build_boe_bi(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),len(emotions)),dtype=float)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                if word in valores:\n",
    "                    for i in range(0,10):\n",
    "                        if BOW[cont_doc,i] == 0:\n",
    "                            BOW[cont_doc,i] = BOW[cont_doc,i] + valores[word][i]\n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "103c92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_bi = build_boe_bi(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "196bd449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 1., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOE_bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "baeb4198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pesado binario\n",
    "grid.fit(BOE_bi,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b0b85444",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_val_bi = build_boe_bi(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "833b71a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[181 216]\n",
      " [ 69 150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.46      0.56       397\n",
      "           1       0.41      0.68      0.51       219\n",
      "\n",
      "    accuracy                           0.54       616\n",
      "   macro avg       0.57      0.57      0.54       616\n",
      "weighted avg       0.61      0.54      0.54       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOE_val_bi)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22eef39",
   "metadata": {},
   "source": [
    "Primero calculamos el número de documentos en los que aparecen las emociones y lo guardamos \n",
    "en un vector para hacer todo más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "3d4f55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = [0]*10\n",
    "\n",
    "for i in range(0,10):\n",
    "    for tr in vec_df:\n",
    "        cont = 0\n",
    "        for word in tr:\n",
    "            if word in valores and valores[word][i] == 1:\n",
    "                cont = 1\n",
    "                break\n",
    "        mat[i] = mat[i] + cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "3104cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW pesado tfidf\n",
    "def build_boe_ti(tr_text,V,dict_indices):\n",
    "    BOW = build_boe(tr_text,V,dict_indices)\n",
    "    \n",
    "    for i in range(len(tr_text)):\n",
    "        for j in range(len(emotions)):\n",
    "            BOW[i][j] = BOW[i][j]*np.log(len(tr_text)/mat[j])\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "0e76c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_ti = build_boe_ti(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "02da2f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.41463271, 0.        , ..., 0.93064287, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 1.41463271, 0.        , ..., 0.93064287, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.70731635, 0.        , ..., 0.93064287, 0.        ,\n",
       "        0.98275511],\n",
       "       ...,\n",
       "       [0.        , 0.70731635, 0.        , ..., 0.93064287, 0.        ,\n",
       "        1.96551022],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.70731635, 0.        , ..., 0.        , 0.        ,\n",
       "        0.98275511]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BOE_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5a24f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOE_ti,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "8eccad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_val_ti = build_boe_ti(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "98e6b540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 45 352]\n",
      " [ 32 187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.11      0.19       397\n",
      "           1       0.35      0.85      0.49       219\n",
      "\n",
      "    accuracy                           0.38       616\n",
      "   macro avg       0.47      0.48      0.34       616\n",
      "weighted avg       0.50      0.38      0.30       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOE_val_ti)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559fc83",
   "metadata": {},
   "source": [
    "|  Pesado  | F1 score de la etiqueta 1| Accuracy  | macro avg\n",
    "|-----------|----------|------------- | -----------\n",
    "| Binario |  0.51  |  0.54    | 0.54\n",
    "| Frecuencia |  0.51  |  0.55 |  0.55\n",
    "| TfIdf |  0.49  |  0.38   | 0.34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f6b0a1",
   "metadata": {},
   "source": [
    "# 3. Recurso Linguístico de Emociones Mexicano "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "734637b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "with open(\"./SEL.txt\",\"r\",encoding='latin-1') as f:\n",
    "        for emotion in f:\n",
    "            doc += [tokenizer.tokenize(emotion)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "f5229985",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [p for p in doc if len(p)==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b2b22f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = doc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4d8d88ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "emociones = {'Alegría':0,'Enojo':1,'Miedo':2, 'Repulsión':3,'Sorpresa':4,'Tristeza':5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "6b7dd86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc = dict()\n",
    "\n",
    "for word in doc:\n",
    "    dicc[word[0]] = [0,0,0,0,0,0]\n",
    "    \n",
    "for word in doc:\n",
    "    dicc[word[0]][emociones[word[2]]] = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c88bff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_boe_fq(tr_text,V,dict_indices):\n",
    "    BOW = np.zeros((len(tr_text),6),dtype=float)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_indices:\n",
    "                if word in dicc:\n",
    "                    for i in range(0,6):\n",
    "                        BOW[cont_doc,i] = BOW[cont_doc,i] + fdist_doc[word]*dicc[word][i]\n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "3cf5b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_fq_sel = build_boe_fq(tr_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "76fcb639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bolsa de palabras\n",
    "BOE_fq_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "70b8828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOE_fq_sel,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "54299994",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOE_val_fq_sel = build_boe_fq(val_text,V,dict_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "1893e403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[352  45]\n",
      " [182  37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.89      0.76       397\n",
      "           1       0.45      0.17      0.25       219\n",
      "\n",
      "    accuracy                           0.63       616\n",
      "   macro avg       0.56      0.53      0.50       616\n",
      "weighted avg       0.59      0.63      0.57       616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOE_val_fq_sel)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450b607",
   "metadata": {},
   "source": [
    "En un comentario aparte, discuta sobre la estrategía que utilizó para incorporar el\n",
    "\"Probability Factor of Affective use\". No más de 5 renglones.\n",
    "\n",
    "\n",
    "Sol: No lo implementé pero tenía la idea agregarlo como unn factor de crecimiento alpha que multiplica la frecuencia de la palabra, con esto estaríamos controlando más el nivel del sentimiento que queremos capturar. Si el PFA es cercano a uno entonces al multiplicar a la frecuencia será casi igual o disminuirá, pero si el PFA es cercano a cero hará muy pequeña la contribución de la palabra, que es lo que esperaríamos ya que si no se relaciona tanto a ese sentimiento no debería contar tanto. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3740a",
   "metadata": {},
   "source": [
    "# 4. ¿Podemos mejorar con bigramas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dd65c",
   "metadata": {},
   "source": [
    "Hacer un experimento dónde concatene una buena BoW según sus experimentos anteri-\n",
    "ores con otra BoW construida a partir de los 1000 bigramas más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ab0a71a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 1, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vamos a considerar la BoW con pesado que utiliza la frecuencia ya que nos dió los mejores resultados\n",
    "BOW_tr_fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b462dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "8a9862ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizaré el siguiente corpus que se realizó en un inicio\n",
    "\n",
    "bigra = list(bigrams(corpus_palabras))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "43f973d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lo', 'peor'),\n",
       " ('peor', 'de'),\n",
       " ('de', 'todo'),\n",
       " ('todo', 'es'),\n",
       " ('es', 'que'),\n",
       " ('que', 'no'),\n",
       " ('no', 'me'),\n",
       " ('me', 'dan'),\n",
       " ('dan', 'por'),\n",
       " ('por', 'un'),\n",
       " ('un', 'tiempo'),\n",
       " ('tiempo', 'y'),\n",
       " ('y', 'luego'),\n",
       " ('luego', 'vuelven'),\n",
       " ('vuelven', 'estoy'),\n",
       " ('estoy', 'hasta'),\n",
       " ('hasta', 'la'),\n",
       " ('la', 'verga'),\n",
       " ('verga', 'de'),\n",
       " ('de', 'estl'),\n",
       " ('estl', 'a'),\n",
       " ('a', 'la'),\n",
       " ('la', 'vga'),\n",
       " ('vga', 'no'),\n",
       " ('no', 'seas'),\n",
       " ('seas', 'mamón'),\n",
       " ('mamón', '45'),\n",
       " ('45', 'putos'),\n",
       " ('putos', 'minutos'),\n",
       " ('minutos', 'después'),\n",
       " ('después', 'me'),\n",
       " ('me', 'dices'),\n",
       " ('dices', 'que'),\n",
       " ('que', 'apenas'),\n",
       " ('apenas', 'sales'),\n",
       " ('sales', 'no'),\n",
       " ('no', 'me'),\n",
       " ('me', 'querías'),\n",
       " ('querías', 'avisar'),\n",
       " ('avisar', 'en'),\n",
       " ('en', '3'),\n",
       " ('3', 'horas'),\n",
       " ('horas', '?'),\n",
       " ('?', '😑'),\n",
       " ('😑', 'considero'),\n",
       " ('considero', 'que'),\n",
       " ('que', 'lo'),\n",
       " ('lo', 'más'),\n",
       " ('más', 'conveniente'),\n",
       " ('conveniente', 'seria'),\n",
       " ('seria', 'que'),\n",
       " ('que', 'lo'),\n",
       " ('lo', 'retes'),\n",
       " ('retes', 'a'),\n",
       " ('a', 'unos'),\n",
       " ('unos', 'vergazos'),\n",
       " ('vergazos', 'mi'),\n",
       " ('mi', 'jelipe'),\n",
       " ('jelipe', '!'),\n",
       " ('!', 'rómpele'),\n",
       " ('rómpele', 'la'),\n",
       " ('la', 'madre'),\n",
       " ('madre', 'a'),\n",
       " ('a', 'ese'),\n",
       " ('ese', 'pinchi'),\n",
       " ('pinchi', 'joto'),\n",
       " ('joto', '!'),\n",
       " ('!', 'el'),\n",
       " ('el', 'marica'),\n",
       " ('marica', 'de'),\n",
       " ('de', 'mi'),\n",
       " ('mi', 'ex'),\n",
       " ('ex', 'me'),\n",
       " ('me', 'tiene'),\n",
       " ('tiene', 'bloqueada'),\n",
       " ('bloqueada', 'de'),\n",
       " ('de', 'todo'),\n",
       " ('todo', 'así'),\n",
       " ('así', 'uno'),\n",
       " ('uno', 'no'),\n",
       " ('no', 'puede'),\n",
       " ('puede', 'admirar'),\n",
       " ('admirar', 'la'),\n",
       " ('la', '\"'),\n",
       " ('\"', 'belleza'),\n",
       " ('belleza', '\"'),\n",
       " ('\"', 'de'),\n",
       " ('de', 'su'),\n",
       " ('su', 'garnacha'),\n",
       " ('garnacha', '😂'),\n",
       " ('😂', 'mujer'),\n",
       " ('mujer', 'despechadaya'),\n",
       " ('despechadaya', 'pinche'),\n",
       " ('pinche', 'amlo'),\n",
       " ('amlo', 'hazle'),\n",
       " ('hazle', 'esta'),\n",
       " ('esta', 'que'),\n",
       " ('que', 'se'),\n",
       " ('se', 'pela'),\n",
       " ('pela', 'la'),\n",
       " ('la', 'loca'),\n",
       " ('loca', '#reynosafollow'),\n",
       " ('#reynosafollow', '#reynosa'),\n",
       " ('#reynosa', 'putos'),\n",
       " ('putos', '.'),\n",
       " ('.', 'no'),\n",
       " ('no', 'tienen'),\n",
       " ('tienen', 'madre'),\n",
       " ('madre', '.'),\n",
       " ('.', 'ambriados'),\n",
       " ('ambriados', 'mantenidos'),\n",
       " ('mantenidos', '.'),\n",
       " ('.', 'ojetes'),\n",
       " ('ojetes', '.'),\n",
       " ('.', 'como'),\n",
       " ('como', 'es'),\n",
       " ('es', 'posible'),\n",
       " ('posible', '.'),\n",
       " ('.', 'mejor'),\n",
       " ('mejor', 'matarlos'),\n",
       " ('matarlos', 'ustedes'),\n",
       " ('ustedes', 'si'),\n",
       " ('si', 'puden'),\n",
       " ('puden', 'andar'),\n",
       " ('andar', 'de'),\n",
       " ('de', 'chanceros'),\n",
       " ('chanceros', 'pero'),\n",
       " ('pero', 'cuidadito'),\n",
       " ('cuidadito', 'y'),\n",
       " ('y', 'seamos'),\n",
       " ('seamos', 'nosotras'),\n",
       " ('nosotras', 'porque'),\n",
       " ('porque', 'luego'),\n",
       " ('luego', 'luego'),\n",
       " ('luego', 'empiezan'),\n",
       " ('empiezan', 'a'),\n",
       " ('a', 'mamar'),\n",
       " ('mamar', 'hijos'),\n",
       " ('hijos', 'de'),\n",
       " ('de', 'la'),\n",
       " ('la', 'chingada'),\n",
       " ('chingada', '.'),\n",
       " ('.', '@usuario'),\n",
       " ('@usuario', 'jajjaja'),\n",
       " ('jajjaja', 'te'),\n",
       " ('te', 'digo'),\n",
       " ('digo', 'esa'),\n",
       " ('esa', 'madre'),\n",
       " ('madre', 'si'),\n",
       " ('si', 'está'),\n",
       " ('está', 'buena'),\n",
       " ('buena', 'ajjaja'),\n",
       " ('ajjaja', 'odio'),\n",
       " ('odio', 'los'),\n",
       " ('los', 'putos'),\n",
       " ('putos', 'trámites'),\n",
       " ('trámites', 'de'),\n",
       " ('de', 'titulación'),\n",
       " ('titulación', '😡'),\n",
       " ('😡', '😡'),\n",
       " ('😡', '😡'),\n",
       " ('😡', 'pero'),\n",
       " ('pero', 'me'),\n",
       " ('me', 'urge'),\n",
       " ('urge', 'la'),\n",
       " ('la', 'precedula'),\n",
       " ('precedula', '.'),\n",
       " ('.', '@usuario'),\n",
       " ('@usuario', 'no'),\n",
       " ('no', 'te'),\n",
       " ('te', 'equivocabas'),\n",
       " ('equivocabas', 'mi'),\n",
       " ('mi', 'madre'),\n",
       " ('madre', 'y'),\n",
       " ('y', 'tu'),\n",
       " ('tu', 'tenían'),\n",
       " ('tenían', 'muchísima'),\n",
       " ('muchísima', 'razón'),\n",
       " ('razón', 'siempre'),\n",
       " ('siempre', 'es'),\n",
       " ('es', 'mejor'),\n",
       " ('mejor', 'lo'),\n",
       " ('lo', 'que'),\n",
       " ('que', 'viene'),\n",
       " ('viene', '💚'),\n",
       " ('💚', '\"'),\n",
       " ('\"', 'no'),\n",
       " ('no', 'me'),\n",
       " ('me', 'importa'),\n",
       " ('importa', 'lo'),\n",
       " ('lo', 'que'),\n",
       " ('que', 'digan'),\n",
       " ('digan', 'esos'),\n",
       " ('esos', 'putos'),\n",
       " ('putos', 'periodistas'),\n",
       " ('periodistas', 'la'),\n",
       " ('la', 'puta'),\n",
       " ('puta', 'que'),\n",
       " ('que', 'los'),\n",
       " ('los', 'pario'),\n",
       " ('pario', 'oh'),\n",
       " ('oh', 'oh'),\n",
       " ('oh', 'oh'),\n",
       " ('oh', 'hay'),\n",
       " ('hay', 'que'),\n",
       " ('que', 'alentar'),\n",
       " ('alentar', 'a'),\n",
       " ('a', 'la'),\n",
       " ('la', 'selección'),\n",
       " ('selección', '\"'),\n",
       " ('\"', 'ok'),\n",
       " ('ok', 'ok'),\n",
       " ('ok', 'está'),\n",
       " ('está', 'bien'),\n",
       " ('bien', 'ya'),\n",
       " ('ya', 'me'),\n",
       " ('me', 'pasé'),\n",
       " ('pasé', 'de'),\n",
       " ('de', 'verga'),\n",
       " ('verga', '.'),\n",
       " ('.', '*'),\n",
       " ('*', 'se'),\n",
       " ('se', 'baja'),\n",
       " ('baja', 'del'),\n",
       " ('del', 'tren'),\n",
       " ('tren', 'del'),\n",
       " ('del', 'mame'),\n",
       " ('mame', '*'),\n",
       " ('*', 'hermosas'),\n",
       " ('hermosas', 'nalguitas'),\n",
       " ('nalguitas', '😘'),\n",
       " ('😘', '😘'),\n",
       " ('😘', 'con'),\n",
       " ('con', 'ganas'),\n",
       " ('ganas', 'de'),\n",
       " ('de', 'mamar'),\n",
       " ('mamar', 'ese'),\n",
       " ('ese', 'culito'),\n",
       " ('culito', 'hermoso'),\n",
       " ('hermoso', 'matías'),\n",
       " ('matías', 'romero'),\n",
       " ('romero', 'oaxaca'),\n",
       " ('oaxaca', '.'),\n",
       " ('.', 'de'),\n",
       " ('de', 'pie'),\n",
       " ('pie', 'y'),\n",
       " ('y', 'con'),\n",
       " ('con', 'ganas'),\n",
       " ('ganas', 'de'),\n",
       " ('de', 'salir'),\n",
       " ('salir', 'adelante'),\n",
       " ('adelante', '.'),\n",
       " ('.', '¡'),\n",
       " ('¡', 'qué'),\n",
       " ('qué', 'orgullo'),\n",
       " ('orgullo', '!'),\n",
       " ('!', 'por'),\n",
       " ('por', 'esto'),\n",
       " ('esto', 'te'),\n",
       " ('te', 'amo'),\n",
       " ('amo', 'mi'),\n",
       " ('mi', 'méxico'),\n",
       " ('méxico', 'por'),\n",
       " ('por', 'tu'),\n",
       " ('tu', 'gente'),\n",
       " ('gente', '…'),\n",
       " ('…', '<url>'),\n",
       " ('<url>', 'me'),\n",
       " ('me', 'caga'),\n",
       " ('caga', 'lo'),\n",
       " ('lo', 'doble'),\n",
       " ('doble', 'moral'),\n",
       " ('moral', 'y'),\n",
       " ('y', 'mustia'),\n",
       " ('mustia', 'que'),\n",
       " ('que', 'es'),\n",
       " ('es', 'la'),\n",
       " ('la', 'gente'),\n",
       " ('gente', 'sí'),\n",
       " ('sí', 'todos'),\n",
       " ('todos', 'somos'),\n",
       " ('somos', 'mierdas'),\n",
       " ('mierdas', 'putas'),\n",
       " ('putas', 'cabrones'),\n",
       " ('cabrones', 'culeros'),\n",
       " ('culeros', '...'),\n",
       " ('...', '¿'),\n",
       " ('¿', 'y'),\n",
       " ('y', '?'),\n",
       " ('?', 'pinche'),\n",
       " ('pinche', '#tritdn'),\n",
       " ('#tritdn', 'tan'),\n",
       " ('tan', 'pendejo'),\n",
       " ('pendejo', '!'),\n",
       " ('!', '!'),\n",
       " ('!', '!'),\n",
       " ('!', 'ya'),\n",
       " ('ya', 'sabian'),\n",
       " ('sabian', 'que'),\n",
       " ('que', 'estos'),\n",
       " ('estos', 'putos'),\n",
       " ('putos', 'son'),\n",
       " ('son', 'mañosos'),\n",
       " ('mañosos', '!'),\n",
       " ('!', '!'),\n",
       " ('!', 'y'),\n",
       " ('y', 'no'),\n",
       " ('no', 'aprenden'),\n",
       " ('aprenden', '!'),\n",
       " ('!', '!'),\n",
       " ('!', '!'),\n",
       " ('!', 'pendejos'),\n",
       " ('pendejos', '!'),\n",
       " ('!', '!'),\n",
       " ('!', '!'),\n",
       " ('!', 'así'),\n",
       " ('así', 'otros'),\n",
       " ('otros', '2'),\n",
       " ('2', 'si'),\n",
       " ('si', 'no'),\n",
       " ('no', 'mejoran'),\n",
       " ('mejoran', '!'),\n",
       " ('!', '!'),\n",
       " ('!', '@usuario'),\n",
       " ('@usuario', '@usuario'),\n",
       " ('@usuario', 'y'),\n",
       " ('y', 'si'),\n",
       " ('si', 'este'),\n",
       " ('este', '#hermano'),\n",
       " ('#hermano', '#hdp'),\n",
       " ('#hdp', '@usuario'),\n",
       " ('@usuario', 'tuyo'),\n",
       " ('tuyo', ';'),\n",
       " (';', 'se'),\n",
       " ('se', 'queda'),\n",
       " ('queda', 'en'),\n",
       " ('en', '#panamá'),\n",
       " ('#panamá', '?'),\n",
       " ('?', 'el'),\n",
       " ('el', '🐷'),\n",
       " ('🐷', 'd'),\n",
       " ('d', '#javidu'),\n",
       " ('#javidu', 'sonreía'),\n",
       " ('sonreía', 'en'),\n",
       " ('en', '#guatemala'),\n",
       " ('#guatemala', 'estaría'),\n",
       " ('estaría', 'mejor'),\n",
       " ('mejor', 'allá'),\n",
       " ('allá', 'porque'),\n",
       " ('porque', 'lo'),\n",
       " ('lo', 'primero'),\n",
       " ('primero', 'que'),\n",
       " ('que', 'busco'),\n",
       " ('busco', 'en'),\n",
       " ('en', 'una'),\n",
       " ('una', 'cuenta'),\n",
       " ('cuenta', 'es'),\n",
       " ('es', 'verificar'),\n",
       " ('verificar', 'si'),\n",
       " ('si', 'son'),\n",
       " ('son', 'domadores'),\n",
       " ('domadores', 'de'),\n",
       " ('de', 'putas'),\n",
       " ('putas', '.'),\n",
       " ('.', 'se'),\n",
       " ('se', 'llevó'),\n",
       " ('llevó', 'mi'),\n",
       " ('mi', 'chamarra'),\n",
       " ('chamarra', 'mi'),\n",
       " ('mi', 'pantalón'),\n",
       " ('pantalón', 'y'),\n",
       " ('y', 'mis'),\n",
       " ('mis', 'tenis'),\n",
       " ('tenis', 'basta'),\n",
       " ('basta', 'que'),\n",
       " ('que', 'cabrona'),\n",
       " ('cabrona', 'es'),\n",
       " ('es', 'me'),\n",
       " ('me', 'tiene'),\n",
       " ('tiene', 'hasta'),\n",
       " ('hasta', 'la'),\n",
       " ('la', 'verga'),\n",
       " ('verga', 'ya'),\n",
       " ('ya', 'que'),\n",
       " ('que', 'se'),\n",
       " ('se', 'compre'),\n",
       " ('compre', 'sus'),\n",
       " ('sus', 'putas'),\n",
       " ('putas', 'cosas'),\n",
       " ('cosas', 'heeelp'),\n",
       " ('heeelp', 'puta'),\n",
       " ('puta', 'madre'),\n",
       " ('madre', 'quiero'),\n",
       " ('quiero', 'suicidarme'),\n",
       " ('suicidarme', 'por'),\n",
       " ('por', 'la'),\n",
       " ('la', 'hdp'),\n",
       " ('hdp', 'de'),\n",
       " ('de', 'la'),\n",
       " ('la', 'gastritis'),\n",
       " ('gastritis', 'no'),\n",
       " ('no', 'hay'),\n",
       " ('hay', 'como'),\n",
       " ('como', 'las'),\n",
       " ('las', 'milanesas'),\n",
       " ('milanesas', 'de'),\n",
       " ('de', 'mi'),\n",
       " ('mi', 'madre'),\n",
       " ('madre', 'que'),\n",
       " ('que', 'cosa'),\n",
       " ('cosa', 'ricaaaaa'),\n",
       " ('ricaaaaa', 'imposible'),\n",
       " ('imposible', 'dejar'),\n",
       " ('dejar', 'de'),\n",
       " ('de', 'mirarte'),\n",
       " ('mirarte', 'y'),\n",
       " ('y', 'de'),\n",
       " ('de', 'contemplarte'),\n",
       " ('contemplarte', 'mi'),\n",
       " ('mi', 'loca'),\n",
       " ('loca', 'pasión'),\n",
       " ('pasión', '...'),\n",
       " ('...', 'ay'),\n",
       " ('ay', 'amigxs'),\n",
       " ('amigxs', 'la'),\n",
       " ('la', 'tristeza'),\n",
       " ('tristeza', 'invade'),\n",
       " ('invade', 'nuestros'),\n",
       " ('nuestros', 'corazones'),\n",
       " ('corazones', 'es'),\n",
       " ('es', 'el'),\n",
       " ('el', 'cinco'),\n",
       " ('cinco', 'pero'),\n",
       " ('pero', 'de'),\n",
       " ('de', 'octubre'),\n",
       " ('octubre', 'a'),\n",
       " ('a', 'la'),\n",
       " ('la', 'puta'),\n",
       " ('puta', 'verga'),\n",
       " ('verga', '.'),\n",
       " ('.', '😞'),\n",
       " ('😞', '😞'),\n",
       " ('😞', '😞'),\n",
       " ('😞', '💔'),\n",
       " ('💔', '('),\n",
       " ('(', 'jajajajaja'),\n",
       " ('jajajajaja', 'que'),\n",
       " ('que', 'pendeja'),\n",
       " ('pendeja', 'la'),\n",
       " ('la', 'dvd'),\n",
       " ('dvd', ')'),\n",
       " (')', 'para'),\n",
       " ('para', 'cada'),\n",
       " ('cada', 'loco'),\n",
       " ('loco', 'siempre'),\n",
       " ('siempre', 'hay'),\n",
       " ('hay', 'una'),\n",
       " ('una', 'loca'),\n",
       " ('loca', 'que'),\n",
       " ('que', 'muerde'),\n",
       " ('muerde', 'la'),\n",
       " ('la', 'boca'),\n",
       " ('boca', 'de'),\n",
       " ('de', 'esas'),\n",
       " ('esas', 'que'),\n",
       " ('que', 'se'),\n",
       " ('se', 'alocan'),\n",
       " ('alocan', '.'),\n",
       " ('.', 'me'),\n",
       " ('me', 'caga'),\n",
       " ('caga', 'ser'),\n",
       " ('ser', 'tan'),\n",
       " ('tan', 'celosa'),\n",
       " ('celosa', 'y'),\n",
       " ('y', 'querer'),\n",
       " ('querer', 'matar'),\n",
       " ('matar', 'a'),\n",
       " ('a', 'cualquier'),\n",
       " ('cualquier', 'niña'),\n",
       " ('niña', 'que'),\n",
       " ('que', 've'),\n",
       " ('ve', 'o'),\n",
       " ('o', 'le'),\n",
       " ('le', 'habla'),\n",
       " ('habla', 'a'),\n",
       " ('a', 'mi'),\n",
       " ('mi', 'novio'),\n",
       " ('novio', '🙃'),\n",
       " ('🙃', 'muéranse'),\n",
       " ('muéranse', 'putas'),\n",
       " ('putas', 'es'),\n",
       " ('es', 'sólo'),\n",
       " ('sólo', 'mío'),\n",
       " ('mío', '@usuario'),\n",
       " ('@usuario', 'son'),\n",
       " ('son', 'putos'),\n",
       " ('putos', 'y'),\n",
       " ('y', 'les'),\n",
       " ('les', 'da'),\n",
       " ('da', 'miedo'),\n",
       " ('miedo', '...'),\n",
       " ('...', 'total'),\n",
       " ('total', 'ya'),\n",
       " ('ya', 'todos'),\n",
       " ('todos', 'saben'),\n",
       " ('saben', 'que'),\n",
       " ('que', 'les'),\n",
       " ('les', 'gusta'),\n",
       " ('gusta', 'por'),\n",
       " ('por', 'atrás'),\n",
       " ('atrás', '...'),\n",
       " ('...', 'al'),\n",
       " ('al', 'menos'),\n",
       " ('menos', 'a'),\n",
       " ('a', 'uno'),\n",
       " ('uno', 'mas'),\n",
       " ('mas', 'que'),\n",
       " ('que', 'al'),\n",
       " ('al', 'otro'),\n",
       " ('otro', '...'),\n",
       " ('...', 'estoy'),\n",
       " ('estoy', 'hasta'),\n",
       " ('hasta', 'la'),\n",
       " ('la', 'verga'),\n",
       " ('verga', 'de'),\n",
       " ('de', 'sus'),\n",
       " ('sus', 'putos'),\n",
       " ('putos', 'tweets'),\n",
       " ('tweets', 'de'),\n",
       " ('de', '\"'),\n",
       " ('\"', 'open'),\n",
       " ('open', 'for'),\n",
       " ('for', 'a'),\n",
       " ('a', 'surprise'),\n",
       " ('surprise', '\"'),\n",
       " ('\"', '✊🏻'),\n",
       " ('✊🏻', '@usuario'),\n",
       " ('@usuario', '@usuario'),\n",
       " ('@usuario', '@usuario'),\n",
       " ('@usuario', 'a'),\n",
       " ('a', 'ti'),\n",
       " ('ti', 't'),\n",
       " ('t', 'da'),\n",
       " ('da', 'pena'),\n",
       " ('pena', 'mostrar'),\n",
       " ('mostrar', 'tu'),\n",
       " ('tu', 'foto'),\n",
       " ('foto', 'por'),\n",
       " ('por', 'tu'),\n",
       " ('tu', 'cara'),\n",
       " ('cara', 'de'),\n",
       " ('de', 'estupido'),\n",
       " ('estupido', 'y'),\n",
       " ('y', 'maricon'),\n",
       " ('maricon', 'que'),\n",
       " ('que', 'tienes'),\n",
       " ('tienes', 've'),\n",
       " ('ve', 'con'),\n",
       " ('con', 'el'),\n",
       " ('el', 'america'),\n",
       " ('america', 'a'),\n",
       " ('a', 'chingar'),\n",
       " ('chingar', 'a'),\n",
       " ('a', 'su'),\n",
       " ('su', 'madre'),\n",
       " ('madre', '@usuario'),\n",
       " ('@usuario', 'luchona'),\n",
       " ('luchona', 'buchona'),\n",
       " ('buchona', 'mamona'),\n",
       " ('mamona', 'y'),\n",
       " ('y', 'sangrona'),\n",
       " ('sangrona', 'y'),\n",
       " ('y', 'copiona'),\n",
       " ('copiona', 'el'),\n",
       " ('el', 'hijo'),\n",
       " ('hijo', 'de'),\n",
       " ('de', 'la'),\n",
       " ('la', 'jefa'),\n",
       " ('jefa', 'de'),\n",
       " ('de', 'mi'),\n",
       " ('mi', 'mamá'),\n",
       " ('mamá', 'está'),\n",
       " ('está', 'bien'),\n",
       " ('bien', 'guapo'),\n",
       " ('guapo', '.'),\n",
       " ('.', 'pero'),\n",
       " ('pero', 'tiene'),\n",
       " ('tiene', '16'),\n",
       " ('16', ':('),\n",
       " (':(', 'y'),\n",
       " ('y', 'una'),\n",
       " ('una', 'novia'),\n",
       " ('novia', '.'),\n",
       " ('.', 'porqué'),\n",
       " ('porqué', 'me'),\n",
       " ('me', 'haces'),\n",
       " ('haces', 'esto'),\n",
       " ('esto', 'diosito'),\n",
       " ('diosito', '@usuario'),\n",
       " ('@usuario', '@usuario'),\n",
       " ('@usuario', 'yo'),\n",
       " ('yo', 'le'),\n",
       " ('le', 'decía'),\n",
       " ('decía', 'a'),\n",
       " ('a', 'marquitos'),\n",
       " ('marquitos', 'pero'),\n",
       " ('pero', 'pues'),\n",
       " ('pues', 'te'),\n",
       " ('te', 'pones'),\n",
       " ('pones', 'el'),\n",
       " ('el', 'saco'),\n",
       " ('saco', 'marica'),\n",
       " ('marica', 'awebo'),\n",
       " ('awebo', 'putos'),\n",
       " ('putos', 'el'),\n",
       " ('el', 'señor'),\n",
       " ('señor', 'zhang'),\n",
       " ('zhang', 'ya'),\n",
       " ('ya', 'me'),\n",
       " ('me', 'dio'),\n",
       " ('dio', 'regalo'),\n",
       " ('regalo', 'en'),\n",
       " ('en', 'clase'),\n",
       " ('clase', 'de'),\n",
       " ('de', 'asia'),\n",
       " ('asia', 'y'),\n",
       " ('y', 'tal'),\n",
       " ('tal', 'vez'),\n",
       " ('vez', 'si'),\n",
       " ('si', 'no'),\n",
       " ('no', 'estuvieras'),\n",
       " ('estuvieras', 'tan'),\n",
       " ('tan', 'violado'),\n",
       " ('violado', 'por'),\n",
       " ('por', 'maricon'),\n",
       " ('maricon', 'no'),\n",
       " ('no', 'criticaras'),\n",
       " ('criticaras', 'como'),\n",
       " ('como', 'vieja'),\n",
       " ('vieja', 'chismosa'),\n",
       " ('chismosa', 'no'),\n",
       " ('no', 'crees'),\n",
       " ('crees', 'sapo'),\n",
       " ('sapo', '?'),\n",
       " ('?', 'yo'),\n",
       " ('yo', 'nunca'),\n",
       " ('nunca', 'uso'),\n",
       " ('uso', 'la'),\n",
       " ('la', 'palabra'),\n",
       " ('palabra', '\"'),\n",
       " ('\"', 'verga'),\n",
       " ('verga', '\"'),\n",
       " ('\"', 'y'),\n",
       " ('y', 'eso'),\n",
       " ('eso', 'no'),\n",
       " ('no', 'significa'),\n",
       " ('significa', 'que'),\n",
       " ('que', 'no'),\n",
       " ('no', 'sea'),\n",
       " ('sea', 'una'),\n",
       " ('una', 'pinche'),\n",
       " ('pinche', 'lépera'),\n",
       " ('lépera', '.'),\n",
       " ('.', 'lo'),\n",
       " ('lo', 'que'),\n",
       " ('que', 'buscamos'),\n",
       " ('buscamos', 'en'),\n",
       " ('en', 'una'),\n",
       " ('una', 'mujer'),\n",
       " ('mujer', 'principalmente'),\n",
       " ('principalmente', 'es'),\n",
       " ('es', 'que'),\n",
       " ('que', 'no'),\n",
       " ('no', 'nos'),\n",
       " ('nos', 'esté'),\n",
       " ('esté', 'chingando'),\n",
       " ('chingando', 'la'),\n",
       " ('la', 'existencia'),\n",
       " ('existencia', 'si'),\n",
       " ('si', 'es'),\n",
       " ('es', 'guapa'),\n",
       " ('guapa', 'o'),\n",
       " ('o', 'no'),\n",
       " ('no', 'al'),\n",
       " ('al', 'final'),\n",
       " ('final', 'vale'),\n",
       " ('vale', 'verga'),\n",
       " ('verga', '.'),\n",
       " ('.', 'pendejos'),\n",
       " ('pendejos', 'pero'),\n",
       " ('pero', 'q'),\n",
       " ('q', 'tal'),\n",
       " ('tal', 'si'),\n",
       " ('si', 'huviera'),\n",
       " ('huviera', 'perdido'),\n",
       " ('perdido', 'el'),\n",
       " ('el', 'puto'),\n",
       " ('puto', 'dl'),\n",
       " ('dl', 'puebla'),\n",
       " ('puebla', 'pinchi'),\n",
       " ('pinchi', 'equipo'),\n",
       " ('equipo', 'mediocre'),\n",
       " ('mediocre', '@usuario'),\n",
       " ('@usuario', 'que'),\n",
       " ('que', 'nuestro'),\n",
       " ('nuestro', 'señor'),\n",
       " ('señor', 'jesus'),\n",
       " ('jesus', 'lo'),\n",
       " ('lo', 'cuide'),\n",
       " ('cuide', 'y'),\n",
       " ('y', 'proteja'),\n",
       " ('proteja', 'y'),\n",
       " ('y', 'nuestra'),\n",
       " ('nuestra', 'madre'),\n",
       " ('madre', 'maria'),\n",
       " ('maria', 'lo'),\n",
       " ('lo', 'guíe'),\n",
       " ('guíe', 'y'),\n",
       " ('y', 'lo'),\n",
       " ('lo', 'mantenga'),\n",
       " ('mantenga', 'bajo'),\n",
       " ('bajo', 'su'),\n",
       " ('su', 'manto'),\n",
       " ('manto', 'sagrado'),\n",
       " ('sagrado', 'su'),\n",
       " ('su', 'santidad'),\n",
       " ('santidad', '.'),\n",
       " ('.', 'buen'),\n",
       " ('buen', 'vieje'),\n",
       " ('vieje', 'jajaja'),\n",
       " ('jajaja', '“'),\n",
       " ('“', 'acaba'),\n",
       " ('acaba', 'la'),\n",
       " ('la', 'prepa'),\n",
       " ('prepa', '”'),\n",
       " ('”', '“'),\n",
       " ('“', 'buchón'),\n",
       " ('buchón', '”'),\n",
       " ('”', 'jajajaja'),\n",
       " ('jajajaja', 'conozco'),\n",
       " ('conozco', 'niñitas'),\n",
       " ('niñitas', 'que'),\n",
       " ('que', 'se'),\n",
       " ('se', 'defienden'),\n",
       " ('defienden', 'mejor'),\n",
       " ('mejor', 'que'),\n",
       " ('que', 'tú'),\n",
       " ('tú', '.'),\n",
       " ('.', 'llégale'),\n",
       " ('llégale', 'a'),\n",
       " ('a', 'la'),\n",
       " ('la', 'verga'),\n",
       " ('verga', 'pinche'),\n",
       " ('pinche', 'puñetas'),\n",
       " ('puñetas', '.'),\n",
       " ('.', 'solo'),\n",
       " ('solo', 'espero'),\n",
       " ('espero', 'que'),\n",
       " ('que', 'hoy'),\n",
       " ('hoy', 'no'),\n",
       " ('no', 'me'),\n",
       " ('me', 'toque'),\n",
       " ('toque', 'el'),\n",
       " ('el', 'mismo'),\n",
       " ('mismo', '#traficogt'),\n",
       " ('#traficogt', 'ee'),\n",
       " ('ee', 'todas'),\n",
       " ('todas', 'las'),\n",
       " ('las', 'putas'),\n",
       " ('putas', 'mañanas'),\n",
       " ('mañanas', 'pinche'),\n",
       " ('pinche', 'mono'),\n",
       " ('mono', 'maricon'),\n",
       " ('maricon', 'muy'),\n",
       " ('muy', 'chingon'),\n",
       " ('chingon', 'haciendo'),\n",
       " ('haciendo', 'llorar'),\n",
       " ('llorar', 'a'),\n",
       " ('a', 'una'),\n",
       " ('una', 'señora'),\n",
       " ('señora', 'mayor'),\n",
       " ('mayor', 'no'),\n",
       " ('no', 'has'),\n",
       " ('has', 'de'),\n",
       " ('de', 'tener'),\n",
       " ('tener', 'madre'),\n",
       " ('madre', 'ojete'),\n",
       " ('ojete', '!'),\n",
       " ('!', '!'),\n",
       " ('!', '!'),\n",
       " ('!', 'putos'),\n",
       " ('putos', 'simios'),\n",
       " ('simios', 'ojalá'),\n",
       " ('ojalá', 'no'),\n",
       " ('no', 'lleguen'),\n",
       " ('lleguen', 'al'),\n",
       " ('al', 'mundial'),\n",
       " ('mundial', 'malditos'),\n",
       " ('malditos', 'hondureños'),\n",
       " ('hondureños', 'miserables'),\n",
       " ('miserables', '.'),\n",
       " ('.', 'che'),\n",
       " ('che', 'si'),\n",
       " ('si', 'el'),\n",
       " ('el', 'relator'),\n",
       " ('relator', 'es'),\n",
       " ('es', 'de'),\n",
       " ('de', 'allboys'),\n",
       " ('allboys', 'ponganlo'),\n",
       " ('ponganlo', 'a'),\n",
       " ('a', 'relatar'),\n",
       " ('relatar', 'a'),\n",
       " ('a', 'los'),\n",
       " ('los', 'putos'),\n",
       " ('putos', 'de'),\n",
       " ('de', 'floresta'),\n",
       " ('floresta', '!'),\n",
       " ('!', 'tremendo'),\n",
       " ('tremendo', 'culo'),\n",
       " ('culo', 'ardido'),\n",
       " ('ardido', 'el'),\n",
       " ('el', 'hijo'),\n",
       " ('hijo', 'de'),\n",
       " ('de', 'puta'),\n",
       " ('puta', '.'),\n",
       " ('.', 'por'),\n",
       " ('por', 'qué'),\n",
       " ('qué', 'putas'),\n",
       " ('putas', 'ponen'),\n",
       " ('ponen', 'a'),\n",
       " ('a', 'benedetto'),\n",
       " ('benedetto', 'de'),\n",
       " ('de', 'inicio'),\n",
       " ('inicio', 'después'),\n",
       " ('después', 'del'),\n",
       " ('del', 'papelón'),\n",
       " ('papelón', 'que'),\n",
       " ('que', 'hizo'),\n",
       " ('hizo', 'con'),\n",
       " ('con', 'perú'),\n",
       " ('perú', '?'),\n",
       " ('?', '@usuario'),\n",
       " ('@usuario', 'igual'),\n",
       " ('igual', 'que'),\n",
       " ('que', 'todos'),\n",
       " ('todos', 'lo'),\n",
       " ('lo', 'políticos'),\n",
       " ('políticos', 'hdp'),\n",
       " ('hdp', '('),\n",
       " ('(', 'hijos'),\n",
       " ('hijos', 'de'),\n",
       " ('de', 'puta'),\n",
       " ('puta', '.'),\n",
       " ('.', 'para'),\n",
       " ('para', 'que'),\n",
       " ('que', 'no'),\n",
       " ('no', 'quede'),\n",
       " ('quede', 'duda'),\n",
       " ('duda', ')'),\n",
       " (')', 'de'),\n",
       " ('de', 'lo'),\n",
       " ('lo', 'que'),\n",
       " ('que', 'se'),\n",
       " ('se', 'roban'),\n",
       " ('roban', 'así'),\n",
       " ('así', 'como'),\n",
       " ('como', 'tu'),\n",
       " ('tu', '.'),\n",
       " ('.', 'no'),\n",
       " ('no', 'te'),\n",
       " ('te', 'mordiste'),\n",
       " ('mordiste', 'la'),\n",
       " ('la', 'lengua'),\n",
       " ('lengua', '?'),\n",
       " ('?', '@usuario'),\n",
       " ('@usuario', 'lo'),\n",
       " ('lo', 'dudó'),\n",
       " ('dudó', 'son'),\n",
       " ('son', 'sus'),\n",
       " ('sus', 'amigos'),\n",
       " ('amigos', 'del'),\n",
       " ('del', 'marica'),\n",
       " ('marica', '@usuario'),\n",
       " ('@usuario', '.'),\n",
       " ('.', 'y'),\n",
       " ('y', 'los'),\n",
       " ('los', 'que'),\n",
       " ('que', 'dieron'),\n",
       " ('dieron', 'domicilio'),\n",
       " ('domicilio', 'falso'),\n",
       " ('falso', 'los'),\n",
       " ('los', 'van'),\n",
       " ('van', 'a'),\n",
       " ('a', 'buscar'),\n",
       " ('buscar', 'en'),\n",
       " ('en', 'la'),\n",
       " ('la', 'sección'),\n",
       " ('sección', 'amarilla'),\n",
       " ('amarilla', '?'),\n",
       " ('?', '🤔'),\n",
       " ('🤔', '🤔'),\n",
       " ('🤔', '🤔'),\n",
       " ('🤔', 'desde'),\n",
       " ('desde', 'que'),\n",
       " ('que', '@usuario'),\n",
       " ('@usuario', '@usuario'),\n",
       " ('@usuario', '@usuario'),\n",
       " ('@usuario', '@usuario'),\n",
       " ('@usuario', 'ya'),\n",
       " ('ya', 'no'),\n",
       " ('no', 'tuitean'),\n",
       " ('tuitean', 'esta'),\n",
       " ('esta', 'red'),\n",
       " ('red', 'social'),\n",
       " ('social', 'vale'),\n",
       " ('vale', 'verga'),\n",
       " ('verga', 'pague'),\n",
       " ('pague', 'infracciones'),\n",
       " ('infracciones', 'del'),\n",
       " ('del', 'bmw'),\n",
       " ('bmw', '...'),\n",
       " ('...', 'y'),\n",
       " ('y', 'lo'),\n",
       " ('lo', 'lleve'),\n",
       " ('lleve', 'a'),\n",
       " ('a', 'verificar'),\n",
       " ('verificar', '😐'),\n",
       " ('😐', 'esta'),\n",
       " ('esta', 'vida'),\n",
       " ('vida', 'loca'),\n",
       " ('loca', '!'),\n",
       " ('!', '😢'),\n",
       " ('😢', '😢'),\n",
       " ('😢', 'regresé'),\n",
       " ('regresé', 'perritos'),\n",
       " ('perritos', 'y'),\n",
       " ('y', 'ando'),\n",
       " ('ando', 'muy'),\n",
       " ('muy', 'cachonda'),\n",
       " ('cachonda', 'asi'),\n",
       " ('asi', 'que'),\n",
       " ('que', 'quiero'),\n",
       " ('quiero', 'verga'),\n",
       " ('verga', 'y'),\n",
       " ('y', 'me'),\n",
       " ('me', 'fue'),\n",
       " ('fue', 'súper'),\n",
       " ('súper', 'verga'),\n",
       " ('verga', 'en'),\n",
       " ('en', 'mi'),\n",
       " ('mi', 'trabajo'),\n",
       " ('trabajo', 'yeeeei'),\n",
       " ('yeeeei', 'los'),\n",
       " ('los', 'días'),\n",
       " ('días', 'buenos'),\n",
       " ('buenos', 'come'),\n",
       " ('come', 'back'),\n",
       " ('back', '😎'),\n",
       " ('😎', 'la'),\n",
       " ('la', 'semana'),\n",
       " ('semana', 'de'),\n",
       " ('de', 'exámenes'),\n",
       " ('exámenes', 'y'),\n",
       " ('y', 'proyectos'),\n",
       " ('proyectos', 'me'),\n",
       " ('me', 'está'),\n",
       " ('está', 'dando'),\n",
       " ('dando', 'en'),\n",
       " ('en', 'la'),\n",
       " ('la', 'madre'),\n",
       " ('madre', '🙄'),\n",
       " ('🙄', 'como'),\n",
       " ('como', 'serás'),\n",
       " ('serás', 'pendejo'),\n",
       " ('pendejo', 'maldito'),\n",
       " ('maldito', 'homosexual'),\n",
       " ('homosexual', 'de'),\n",
       " ('de', 'cagada'),\n",
       " ('cagada', 'ojalá'),\n",
       " ('ojalá', 'putin'),\n",
       " ('putin', 'llegue'),\n",
       " ('llegue', 'a'),\n",
       " ('a', 'tener'),\n",
       " ('tener', 'el'),\n",
       " ('el', 'control'),\n",
       " ('control', 'del'),\n",
       " ('del', 'nuevo'),\n",
       " ('nuevo', '…'),\n",
       " ('…', 'si'),\n",
       " ('si', 'la'),\n",
       " ('la', 'vida'),\n",
       " ('vida', 'me'),\n",
       " ('me', 'da'),\n",
       " ('da', 'la'),\n",
       " ('la', 'oportunidad'),\n",
       " ('oportunidad', 'de'),\n",
       " ('de', 'ser'),\n",
       " ('ser', 'madre'),\n",
       " ('madre', 'me'),\n",
       " ('me', 'asegúrare'),\n",
       " ...]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2023c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdistb = nltk.FreqDist(bigra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9949e0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('!', '!'): 591, ('la', 'verga'): 419, ('a', 'la'): 391, ('de', 'la'): 311, ('@usuario', '@usuario'): 298, ('que', 'no'): 221, ('que', 'me'): 220, ('la', 'madre'): 194, ('los', 'putos'): 175, ('en', 'la'): 174, ...})"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdistb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "bce8b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = sortFreqDict(fdistb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "1bf2d6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = K[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "00244d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_b = dict()\n",
    "cont = 0 \n",
    "\n",
    "for weight, word in K:\n",
    "    dict_b[word] = cont\n",
    "    cont += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "3b7d14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW con pesado frecuencia para bigramas\n",
    "\n",
    "def build_bow_fq_b(tr_text,K,dict_b):\n",
    "    BOW = np.zeros((len(tr_text),len(K)),dtype=int)\n",
    "    cont_doc = 0\n",
    "    for tr in tr_text:\n",
    "        fdist_doc = nltk.FreqDist(tokenizer.tokenize(tr))\n",
    "        for word in fdist_doc:\n",
    "            if word in dict_b:\n",
    "                BOW[cont_doc,dict_b[word]] = fdist_doc[word]\n",
    "        cont_doc += 1\n",
    "    \n",
    "    return BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "655f8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_bi_fq = build_bow_fq_b(tr_text,K,dict_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d162e3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(class_weight='balanced'), n_jobs=4,\n",
       "             param_grid={'C': [0.05, 0.12, 0.25, 0.5, 1, 2, 4]},\n",
       "             scoring='f1_macro')"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(BOW_bi_fq,tr_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b380eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_val_fq_bi = build_bow_fq_b(val_text,K,dict_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "5476db0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[397   0]\n",
      " [219   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78       397\n",
      "           1       0.00      0.00      0.00       219\n",
      "\n",
      "    accuracy                           0.64       616\n",
      "   macro avg       0.32      0.50      0.39       616\n",
      "weighted avg       0.42      0.64      0.51       616\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ernesto/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid.predict(BOW_val_fq_bi)\n",
    "\n",
    "p, r, f, _ = precision_recall_fscore_support(val_y, y_pred, average=\"macro\", pos_label=1)\n",
    "\n",
    "print(confusion_matrix(val_y,y_pred))\n",
    "print(metrics.classification_report(val_y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39419dc",
   "metadata": {},
   "source": [
    "Hacer un experimento con las Bolsas de Emociones, Bolsa de Palabras y Bolsa de Bi-\n",
    "gramas; usted elige las dimensionalidades. Para construir la representación final del\n",
    "\n",
    "documento utilice la concatenación de las representaciones según sus observaciones\n",
    "(e.g., Bolsa de Palabras + Bolsa de Bigramas + Bolsa de Sentimientos de Canadá + Bolsa\n",
    "de Sentimientos de Grigori), y aliméntelas a un SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ddf62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6128d22d",
   "metadata": {},
   "source": [
    "Elabore conclusiones sobre toda esta Tarea, incluyendo observaciones, comentarios y\n",
    "posibles mejoras futuras. Discuta el comportamiento de la BoW de usar solo palabras\n",
    "a integrar bigramas, y luego a integrar todo ¿ayudó? o ¿empeoró?. Discuta también\n",
    "brevemente el costo computacional de los experimentos ¿Valió la Pena tener todo?. Sea\n",
    "breve: todo en NO más de dos párrafos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6d0c0d",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "\n",
    "En el punto 1 podemos notar que de entre los 6 modelos de pesados que utilizamos, los que mejores resultados mostrarón fueron los de pesado con frecuencia y pesado binario, con valores de accurary entre 0.81 y 0.82. Por otro lado mientras el pesado tfidf obtuvo resultados mucho más bajo, esto se debe (creo) a que estamos analizando tweets, las oraciones son muy pequeñas y utilizan muy pocos términos por lo que el factor idf casi no penaliza. Este tipo de pesado se esperaría para textos muy muy grandes y una gran cantidad de ellas. Pudimos notar que con la normalización los valores para los pesados binarios y de frecuencia se mantuvieron y para los de tfidf se incrementó un poco. Al variar el número de palabras a considerar obtuvimos mejores resultados con solo 1000 palabras, esto se debe a que estas deben ser las que dsicriminan mejor si un tweet es agresivo o no, y las siguientes pueden ser palabras que se repiten mucho pero no discriminan. \n",
    "\n",
    "Utilizando bolsa de emociones obtuvimos resultados más bajos que con la bolsa de palabras. A mi entender, esto es lo que se debería esperar ya que estamos representando nuestro documento con un vector de dimensión muchísima menor, al restringirnos a las emociones. De modo que estamos capturando unn número menor de carácterísticas. En ese caso el pesado que mejor se comportó fue el de frecuencia, y es que a comparación del binario si tuvieramos en un tweet una palabra mala que se repite mucho solo sería capturada una vez, mientras que para el de frecuencia se caputraría su podera contribución al repetirse mucho. Finalmente con la bolsa de palabras se obtienen resultados parecidos o un poco más bajo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
